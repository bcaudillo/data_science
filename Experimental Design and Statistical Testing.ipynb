{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1dea870-ba45-4ca0-ac4f-46f7ba1aca9a",
   "metadata": {},
   "source": [
    "# Scenario 1\n",
    "\n",
    "The product team says: \"We think the new algorithm will make people listen more because it's better at finding music they'll like. Can you test if it works?\"\n",
    "\n",
    "State both null and alternative hypotheses precisely. Consider these potential hypotheses:\n",
    "\n",
    "## H₀: The new algorithm does not increase listening time\n",
    "## H₁: The new algorithm increases listening time\n",
    "\n",
    "## Reflection Questions\n",
    "Reflection questions to for clear problem definition guides the entire investigation:\n",
    "\n",
    "1. What makes this initial request problematic for hypothesis testing?\n",
    "2. How would you refine this into a specific, measurable research question?\n",
    "3. What additional information would you need from the product team?\n",
    "4. Would a one-sided test or two-sided test be best in this scenario? Why?\n",
    "5. What are the potential risks of choosing a one-sided test?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee54fff-1fb2-4687-a578-06e7b02c164c",
   "metadata": {},
   "source": [
    "# Reflection Response\n",
    "1. The initial request is problematic for hypothesis testing because it's too vague and lacks clear, measurable outcomes. For example, the phrase “make people listen more” doesn’t specify how listening is being measured, whether by total time, sessions, or another metric. Additionally, the idea that the algorithm finds music users “like” implies user satisfaction, but no qualitative or behavioral data is provided to support that. Without defining success in measurable terms, it's impossible to construct a valid testable hypothesis.\n",
    "\n",
    "2. A more specific research question could be:\n",
    "Does the new algorithm increase the average daily listening time per user compared to the existing algorithm over a two-week period?\n",
    "\n",
    "4. Additional information I would request includes:\n",
    "When the new algorithm was released so I can query the correct timeframe. I’d also want to explore listening patterns across multiple metrics like session time, session count, and total listening time per user. Since this would require an A/B test, we would need to roll out the new algorithm to only a portion of users.\n",
    "\n",
    "\n",
    "6. A two-sided test would be more appropriate because it allows us to detect whether the new algorithm has any significant impact—positive or negative—on user listening behavior.\n",
    "\n",
    "7. The risk of using a one-sided test is that it only checks for a positive effect. If the update actually decreases listening time, a one-sided test might miss it entirely. This could lead to poor decisions, especially if users react negatively to the change.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779be1bb-6e44-4624-aa5c-9dd56583cbf1",
   "metadata": {},
   "source": [
    "# Scenario 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eefbe177-1fe0-461f-be18-d326fb44bd9c",
   "metadata": {},
   "source": [
    "The platform has 100 million active users worldwide across different subscription types (free and premium). Given here is Spotify's user segmentation:\n",
    "\n",
    "Premium subscribers: 40% of user base\n",
    "Free tier users: 60% of user base\n",
    "Geographic distribution: 30% US, 40% Europe, 30% Rest of World\n",
    "Device usage: 70% mobile, 20% desktop, 10% other\n",
    "\n",
    "## Reflection Questions\n",
    "\n",
    "1. What factors might affect listening time that need to be controlled for?\n",
    "There are a few things that could impact listening time, like whether a user is on the free or premium tier, premium users probably listen more since they don’t have ads. Region might matter too since people in different time zones or cultures might use the platform differently. Device type could also play a role; people on mobile might listen in shorter bursts compared to desktop users.\n",
    "\n",
    "3. How would you ensure representative sampling across user segments?\n",
    "I’d want to make sure we’re pulling users from each of the major segments, so the same proportions of premium vs. free, geographic regions, and device types. A sampling approach would probably make the most sense here so no group is over or underrepresented.\n",
    "\n",
    "4. What potential biases could emerge from improper sampling?\n",
    "If we don’t sample correctly, we could draw the wrong conclusions. For example, if most of our sample ends up being premium users from Europe using mobile, we might assume the algorithm works better than it actually does, because that group might naturally have longer listening sessions. That kind of bias could lead us to roll out a change that doesn’t perform as well for the rest of the user base.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca4d3b4-79d4-41df-a9a2-81d3d733811a",
   "metadata": {},
   "source": [
    "# Sceanrio 3\n",
    "\n",
    "The product team says implementing the new algorithm will require a lot of engineering resources and costs so the change must be significant.\n",
    "\n",
    "## Reflection Questions\n",
    "1. How would the implementation cost affect your choice of:\n",
    "\n",
    "Significance level (α):\n",
    "Since the change is expensive, I would lower the alpha level, maybe from 0.05 to 0.01. That way, we reduce the chance of a false positive. We want to be very confident in the results before moving forward with something that requires heavy investment.\n",
    "\n",
    "Minimum effect size:\n",
    "I would raise the minimum effect size because a small improvement probably does not justify the cost. We need to define what size of improvement actually moves the needle for the business.\n",
    "\n",
    "Required statistical power:\n",
    "I would aim for higher power, like 90 percent, to make sure we are not missing a real effect. If we are going to invest serious engineering effort, we want to feel confident that we are detecting true impact.\n",
    "\n",
    "2. What specific business metrics would you need to define \"significant improvement\"?\n",
    "I would look at things like average daily listening time per user, session frequency, or churn rate. If the new algorithm also increases retention for premium users, maybe even average revenue per user. Basically, any metric that ties directly to user engagement and long-term value.\n",
    "\n",
    "3. What secondary metrics could be monitored for potential negative impacts?\n",
    "I would track user retention, skip rate, time between sessions, support tickets, and app uninstall rates. An increase in listening time alone is not enough. We want to make sure the new algorithm is actually improving the experience, not just keeping people stuck or frustrated.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168b9286-5bae-4448-b489-8b878ae35a76",
   "metadata": {},
   "source": [
    "# Scenario 4\n",
    "After running your experiment and collecting the appropriate data for one week you observe the following metrics:\n",
    "\n",
    "- Weekly listening time (primary metric)\n",
    "- Control group (n=500,000):\n",
    "    - Mean: 118.5 minutes\n",
    "    - Standard deviation: 44.2 minutes\n",
    "    - 95% Confidence Interval: [117.8, 119.2] minutes\n",
    "- Treatment group (n=500,000):\n",
    "    - Mean: 122.3 minutes\n",
    "    - Standard deviation: 43.8 minutes\n",
    "    - 95% Confidence Interval: [121.6, 123.0] minutes\n",
    "\n",
    "## Reflection Questions\n",
    "1. How would you interpret these confidence intervals in non-technical language?\n",
    "The confidence interval gives us a range where we think the true average listening time for each group likely falls. For the control group, we are pretty confident the real average is somewhere between 117.8 and 119.2 minutes. For the treatment group, it is between 121.6 and 123.0 minutes. These ranges help us understand how much the averages might vary if we ran the experiment again with a new group of users.\n",
    "\n",
    "2. What does the overlap or lack of overlap between confidence intervals tell us?\n",
    "Since there is no overlap between the two confidence intervals, it suggests that the difference we are seeing is probably real and not just due to random chance. In other words, the treatment group is likely listening more because of the new algorithm. This is a good early sign that the change is having a meaningful effect."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f4a2bb4-9b8b-42e4-a7ae-f8348aa13d18",
   "metadata": {},
   "source": [
    "# Scenario 5\n",
    "### Scenario 1: \n",
    "Your test comparing the new and old playlist algorithms returns the following results:\n",
    "\n",
    "p-value = 0.047\n",
    "Average listening time increase: 8 minutes per week\n",
    "Sample size: 500,000 users per group\n",
    "Test duration: 2 weeks\n",
    "\n",
    "### Scenario 2: \n",
    "Your test results show:\n",
    "\n",
    "p-value = 0.08 (above traditional 0.05 threshold)\n",
    "Effect size: 12 minutes increased listening time per week\n",
    "High variance in the data\n",
    "Strong positive trends in user satisfaction metrics\n",
    "\n",
    "## Reflection Questions\n",
    "1. How would you explain this p-value to non-technical stakeholders? What common misinterpretations should you help them avoid? What other factors might you include?\n",
    "I would explain that the p-value tells us how likely it is that the difference we saw in listening time happened just by chance. A p-value of 0.047 means there is about a 4.7 percent chance the increase we saw is due to random variation and not because the new algorithm actually caused it. Since this is under the typical threshold of 5 percent, we usually consider the result statistically significant, which means we are fairly confident the change is real. A small p-value just means the result is probably not due to chance, but it does not tell us if the change is valuable to the business. I would also caution against thinking that passing the 0.05 cutoff automatically means we should launch the new algorithm. We need to look at other things too, like how meaningful the 8-minute increase is and what it would cost to implement the change.\n",
    "\n",
    "2. If running the same test with 5 million users per group returned a p-value of 0.001, but showed only a 2-minute increase in listening time, how would this change your interpretation?\n",
    "A very low p-value like 0.001 tells us the result is almost certainly not due to chance, but the actual improvement is really small. With such a large sample, even tiny differences can appear significant. So I would ask whether a 2-minute increase per week is actually valuable to the business or noticeable to users. Just because the result is statistically significant does not mean it is worth acting on.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069cbb28-91a2-4310-bf51-895575707d4b",
   "metadata": {},
   "source": [
    "1. How would you approach making a recommendation in this case?\n",
    "Even though the p-value is 0.08, which is slightly above the usual threshold of 0.05, the effect size is large and user satisfaction is improving. That makes me think this result is still worth considering. The high variance may be making the result less clear, but the positive trends suggest the new algorithm is doing something good. I would recommend running a follow-up test or extending this one to gather more evidence. If those results hold, it might still make sense to move forward.\n",
    "\n",
    "2. How would you handle stakeholder pressure to make a yes or no decision based solely on statistical significance?\n",
    "I would remind the team that the p-value is just one piece of the puzzle. In this case, we are seeing a strong increase in listening time and users seem to like the change. I would shift the conversation to whether we have enough evidence to feel confident in the decision, not just whether we passed an arbitrary threshold. If there is hesitation, I would suggest more testing to build confidence before taking action.\n",
    "\n",
    "3. What additional data or analyses might help provide more context for decision-making?\n",
    "I would look at how different user segments responded, like premium versus free users, or different regions and devices. I would also track long-term engagement metrics like retention, not just listening time. Running the test longer or with more users could also help smooth out the variance and give us a clearer picture of whether the results are consistent.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a185548f-c8e2-4f41-b6ee-4d65b96e0ae2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9180a0c2-b8b1-46eb-ad0e-b44c4b62eed1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
