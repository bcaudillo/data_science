{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1dc50a11-f641-4e06-bf18-12655fd89c0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/06 12:07:52 WARN Utils: Your hostname, MacBook-Air-6.local resolves to a loopback address: 127.0.0.1; using 192.168.7.45 instead (on interface en0)\n",
      "25/05/06 12:07:52 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/05/06 12:07:53 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/05/06 12:08:09 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['JAVA_HOME'] = '/Library/Java/JavaVirtualMachines/jdk-20.jdk/Contents/Home' \n",
    "import findspark\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from py4j.protocol import Py4JError # Import the Py4JError exception class\n",
    "\n",
    "# Initialize findspark to add PySpark to the system path\n",
    "findspark.init()\n",
    "\n",
    "\n",
    "# Create a SparkConf object\n",
    "conf = SparkConf().setAppName(\"SparkContextDemo\").setMaster(\"local[*]\")\n",
    "\n",
    "\n",
    "# Create a SparkContext\n",
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83c20b7d-98ef-416c-baa7-41adb9e122f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Application Name: SparkContextDemo\n",
      "Master URL: local[*]\n",
      "SparkConf: [('spark.app.name', 'SparkContextDemo'), ('spark.executor.id', 'driver'), ('spark.driver.extraJavaOptions', '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false'), ('spark.driver.port', '53031'), ('spark.rdd.compress', 'True'), ('spark.serializer.objectStreamReset', '100'), ('spark.driver.host', '192.168.7.45'), ('spark.master', 'local[*]'), ('spark.submit.pyFiles', ''), ('spark.submit.deployMode', 'client'), ('spark.app.id', 'local-1746554874161'), ('spark.app.startTime', '1746554873312'), ('spark.ui.showConsoleProgress', 'true'), ('spark.executor.extraJavaOptions', '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false'), ('spark.app.submitTime', '1746554872980')]\n",
      "Default Parallelism: 8\n",
      "Default Minimum Partitions: 2\n",
      "Spark Home: None\n",
      "Spark User: brandoncaudillo\n",
      "Spark Version: 3.5.0\n",
      "Python Version: 3.12\n",
      "Registered RDDs: {}\n",
      "Available Resources: Map()\n",
      "Current Log Level: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum of RDD: 45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Demonstrate various SparkContext attributes and settings\n",
    "\n",
    "\n",
    "# Application Name\n",
    "print(\"Application Name:\", sc.appName)\n",
    "\n",
    "\n",
    "# Master URL\n",
    "print(\"Master URL:\", sc.master)\n",
    "\n",
    "\n",
    "# SparkConf\n",
    "print(\"SparkConf:\", sc.getConf().getAll())\n",
    "\n",
    "\n",
    "# Default Parallelism\n",
    "print(\"Default Parallelism:\", sc.defaultParallelism)\n",
    "\n",
    "\n",
    "# Default Minimum Partitions\n",
    "print(\"Default Minimum Partitions:\", sc.defaultMinPartitions)\n",
    "\n",
    "\n",
    "# Spark Home\n",
    "print(\"Spark Home:\", sc.sparkHome)\n",
    "\n",
    "\n",
    "# Spark User\n",
    "print(\"Spark User:\", sc.sparkUser())\n",
    "\n",
    "\n",
    "# Spark Version\n",
    "print(\"Spark Version:\", sc.version)\n",
    "\n",
    "\n",
    "# Python Version\n",
    "print(\"Python Version:\", sc.pythonVer)\n",
    "\n",
    "\n",
    "# Job Progress Listener\n",
    "#print(\"Job Progress Listener:\", sc._jsc.sc().jobProgressListener()) # This line is causing the error\n",
    "try:\n",
    "   #print(\"Job Progress Listener:\", sc._jsc.sc().jobProgressListener())  # Try the old way - This is commented out as it is known to cause an error.\n",
    "   # The JobProgressListener is an internal implementation detail and not meant for direct access\n",
    "   # Instead of directly accessing the listener, you can monitor job progress using Spark UI or other methods\n",
    "   pass  # Placeholder for alternative approach\n",
    "except Py4JError:  # Now that Py4JError is imported, this except block should work correctly\n",
    "   from pyspark.status import SparkStatusTracker\n",
    "   statusTracker = SparkStatusTracker(sc)\n",
    "   print(\"Job Progress Listener:\", statusTracker.getActiveJobsIds())  # Use SparkStatusTracker instead\n",
    "\n",
    "\n",
    "# List of Registered RDDs\n",
    "print(\"Registered RDDs:\", sc._jsc.getPersistentRDDs())\n",
    "\n",
    "\n",
    "# Available Resources\n",
    "print(\"Available Resources:\", sc._jsc.sc().resources())\n",
    "\n",
    "\n",
    "# Modify Log Level\n",
    "sc.setLogLevel(\"WARN\")\n",
    "\n",
    "\n",
    "# Instead of using sc.getLogLevel(), you can access the log level from SparkConf:\n",
    "log_level = sc.getConf().get(\"spark.logConf\")\n",
    "print(\"Current Log Level:\", log_level)\n",
    "\n",
    "\n",
    "# Alternatively, you can check the current log level using the following approach:\n",
    "# import logging\n",
    "# log_level = logging.getLogger(\"py4j\").getEffectiveLevel()\n",
    "# print(\"Current Log Level:\", logging.getLevelName(log_level))\n",
    "\n",
    "\n",
    "# Create a simple RDD and perform an action\n",
    "rdd = sc.parallelize(range(10))\n",
    "sum_result = rdd.sum()\n",
    "print(\"Sum of RDD:\", sum_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e0a9f74d-bff0-4463-88c9-ab4d276e09b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd5a948-1d00-4c01-822c-b10786fc2a70",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
