{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c2ba862",
   "metadata": {},
   "source": [
    "## Machine Learning Cheatsheet: The Big Picture- Math Side\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a29504",
   "metadata": {},
   "source": [
    "# I. The Foundation: Data, Cloud, and Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee60d82b",
   "metadata": {},
   "source": [
    "### I. The Foundation: Data, Cloud, and Statistics\n",
    "\n",
    "* **Cloud Computing & Big Data:**\n",
    "    * **Concept:** Cloud computing provides scalable computational power (CPUs, GPUs) and storage, essential for handling \"Big Data.\" PySpark, Numpy, Pandas, and SQL are tools for manipulating and querying these large datasets.\n",
    "    * **Mathematical Relevance:** While not directly mathematical *equations* for cloud computing itself, understanding distributed processing (PySpark) implies parallelization of computations. For instance, matrix operations in Numpy/Pandas are optimized for speed, which is crucial when scaled to big data.\n",
    "    * **AI Workflow:** The structured process of an AI project. Data preparation often involves mathematical transformations.\n",
    "    * **Generative AI:** Involves complex probabilistic models and neural networks (which we'll discuss later) to generate new data, often relying on high-dimensional probability distributions.\n",
    "\n",
    "* **Inferential Statistics:**\n",
    "    * **Concept:** Making educated guesses about a population from a sample, quantifying uncertainty.\n",
    "    * **Why it Matters:** Provides the theoretical basis for evaluating model significance, confidence in predictions, and comparing different models or treatments (e.g., A/B testing).\n",
    "    * **Key Mathematical Ideas:**\n",
    "        * **Probability Distributions:** Describing the likelihood of outcomes.\n",
    "            * **Normal Distribution (Gaussian):**\n",
    "                $$P(x) = \\frac{1}{\\sigma\\sqrt{2\\pi}} e^{-\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^2}$$\n",
    "                (where $\\mu$ is mean, $\\sigma$ is standard deviation). Visualized as a bell curve.\n",
    "            * **Binomial Distribution:** For discrete outcomes (e.g., success/failure).\n",
    "                $$P(k; n, p) = \\binom{n}{k} p^k (1-p)^{n-k}$$\n",
    "        * **Central Limit Theorem:** States that the distribution of sample means approaches a normal distribution as the sample size increases, regardless of the population's distribution. Foundation for many statistical tests.\n",
    "        * **Hypothesis Testing (A/B Testing):** Involves setting up null ($H_0$) and alternative ($H_1$) hypotheses and using p-values to decide whether to reject $H_0$.\n",
    "            * **P-value:** The probability of observing data as extreme as, or more extreme than, the observed data, assuming the null hypothesis is true.\n",
    "            * **T-test (for comparing means):**\n",
    "                $$t = \\frac{\\bar{x}_1 - \\bar{x}_2}{\\sqrt{\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}}}$$\n",
    "        * **Confidence Intervals:** A range of values, derived from sample statistics, that is likely to contain an unknown population parameter. Example for a mean: $\\bar{x} \\pm Z_{\\alpha/2} \\frac{\\sigma}{\\sqrt{n}}$.\n",
    "    * **Visualization:** Histograms showing distributions, box plots for comparing groups, bell curves for normal distributions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a4a3a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "82d5f7c3",
   "metadata": {},
   "source": [
    "# II. Core Machine Learning: Prediction & Insights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d5540a0",
   "metadata": {},
   "source": [
    "### II. Core Machine Learning: Prediction & Insights\n",
    "\n",
    "* **Regression:**\n",
    "    * **Concept:** Predicting a *continuous* numerical value. The goal is to find a function that best maps input features to the target output.\n",
    "    * **Why it Matters:** Predictive analytics for numerical outcomes.\n",
    "    * **Key Models & Optimization:**\n",
    "        * **Linear Regression:**\n",
    "            * **Model:**\n",
    "                $$y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\dots + \\beta_n x_n + \\epsilon$$\n",
    "                (where $\\beta_i$ are coefficients, $\\epsilon$ is error).\n",
    "            * **Objective/Cost Function (Mean Squared Error - MSE):** The model \"learns\" by minimizing this function.\n",
    "                $$J(\\beta) = \\frac{1}{m} \\sum_{i=1}^{m} (y^{(i)} - \\hat{y}^{(i)})^2 = \\frac{1}{m} \\sum_{i=1}^{m} \\left(y^{(i)} - \\left(\\beta_0 + \\sum_{j=1}^{n} \\beta_j x_j^{(i)}\\right)\\right)^2$$\n",
    "                * **Visualization:** A line or hyperplane trying to fit through the data points. The vertical distance from each point to the line represents the error.\n",
    "            * **Optimization:**\n",
    "                * **Normal Equation (Closed Form Solution):** For simple linear regression, the coefficients can be found directly:\n",
    "                    $$\\hat{\\beta} = (X^T X)^{-1} X^T y$$\n",
    "                    This is computationally expensive for very large datasets ($O(n^3)$ where $n$ is features).\n",
    "                * **Gradient Descent:** An iterative optimization algorithm. It repeatedly adjusts the parameters in the direction opposite to the gradient of the cost function.\n",
    "                    * **Update Rule:**\n",
    "                        $$\\beta_j := \\beta_j - \\alpha \\frac{\\partial}{\\partial \\beta_j} J(\\beta)$$\n",
    "                        where:\n",
    "                        $$\\frac{\\partial}{\\partial \\beta_j} J(\\beta) = \\frac{2}{m} \\sum_{i=1}^{m} \\left(\\left(\\beta_0 + \\sum_{k=1}^{n} \\beta_k x_k^{(i)}\\right) - y^{(i)}\\right) x_j^{(i)} \\quad \\text{(for } j \\ge 1\\text{)}$$                       $$\\frac{\\partial}{\\partial \\beta_0} J(\\beta) = \\frac{2}{m} \\sum_{i=1}^{m} \\left(\\left(\\beta_0 + \\sum_{k=1}^{n} \\beta_k x_k^{(i)}\\right) - y^{(i)}\\right)$$\n",
    "                    * $\\alpha$ is the **learning rate**, controlling step size.\n",
    "                    * **Visualization:** Imagine a ball rolling down a bowl (the cost function surface) to find the lowest point (the minimum). The learning rate determines how big each step is.\n",
    "        * **Logistic Regression:**\n",
    "            * **Model:** Predicts the probability of a binary outcome using the sigmoid function.\n",
    "                $$\\hat{y} = P(Y=1|X) = \\sigma(z) = \\frac{1}{1 + e^{-z}}$$\n",
    "                where $z = \\beta_0 + \\sum_{j=1}^{n} \\beta_j x_j$.\n",
    "            * **Objective/Cost Function (Binary Cross-Entropy Loss):** Minimizing this loss function pushes probabilities towards 0 for negative classes and 1 for positive classes.\n",
    "                $$J(\\beta) = -\\frac{1}{m} \\sum_{i=1}^{m} [y^{(i)} \\log(\\hat{y}^{(i)}) + (1-y^{(i)}) \\log(1-\\hat{y}^{(i)})]$$\n",
    "            * **Optimization:** Typically uses Gradient Descent (or more advanced variants like SGD, Adam). The partial derivatives for updating $\\beta$ are derived from the cross-entropy loss.\n",
    "            * **Visualization:** An S-shaped (sigmoid) curve separating two classes, with a decision boundary where probability is 0.5.\n",
    "    * **Model Selection & Regularization:**\n",
    "        * **Overfitting:** When a model learns the training data too well, failing to generalize to new data.\n",
    "        * **Regularization (Lasso, Ridge):** Adds a penalty to the cost function to prevent large coefficients, making models simpler and less prone to overfitting.\n",
    "            * **Ridge Regression (L2 Regularization):**\n",
    "                $$J_{Ridge}(\\beta) = MSE(\\beta) + \\lambda \\sum_{j=1}^{n} \\beta_j^2$$\n",
    "            * **Lasso Regression (L1 Regularization):**\n",
    "                $$J_{Lasso}(\\beta) = MSE(\\beta) + \\lambda \\sum_{j=1}^{n} |\\beta_j|$$\n",
    "            * $\\lambda$ is the **regularization parameter**, controlling the strength of the penalty.\n",
    "            * **Visualization:** Imagine the cost function contour plot. Regularization adds a constraint that shrinks the acceptable parameter space, pushing coefficients towards zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c27a96a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7c53a4a9",
   "metadata": {},
   "source": [
    "# III  **Introduction to Machine Learning:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563248e8",
   "metadata": {},
   "source": [
    "\n",
    "* **Introduction to Machine Learning:**\n",
    "    * **Statistical Learning Theory:** Focuses on generalization error (how well a model performs on unseen data).\n",
    "    * **Metrics:** Crucial for evaluating model performance.\n",
    "        * **Classification:**\n",
    "            * **Accuracy:** $\\frac{\\text{Number of correct predictions}}{\\text{Total number of predictions}}$\n",
    "            * **Precision:** $\\frac{TP}{TP+FP}$ (True Positives / (True Positives + False Positives))\n",
    "            * **Recall (Sensitivity):** $\\frac{TP}{TP+FN}$ (True Positives / (True Positives + False Negatives))\n",
    "            * **F1-Score:** Harmonic mean of precision and recall: $2 \\cdot \\frac{\\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}}$\n",
    "            * **ROC Curve & AUC:** Visualizes classifier performance across various threshold settings. AUC (Area Under the Curve) quantifies overall performance.\n",
    "            * **Visualization:** Confusion Matrix, ROC curves.\n",
    "        * **Regression:**\n",
    "            * **Mean Squared Error (MSE):**\n",
    "                $$MSE = \\frac{1}{m} \\sum_{i=1}^{m} (y^{(i)} - \\hat{y}^{(i)})^2$$\n",
    "            * **Root Mean Squared Error (RMSE):** $\\sqrt{MSE}$\n",
    "            * **R-squared ($R^2$):**\n",
    "                $$R^2 = 1 - \\frac{\\sum (y^{(i)} - \\hat{y}^{(i)})^2}{\\sum (y^{(i)} - \\bar{y})^2}$$\n",
    "                Represents the proportion of variance in the dependent variable that is predictable from the independent variables.\n",
    "    * **Decision Trees:**\n",
    "        * **Concept:** Tree-like structures where each internal node represents a test on an attribute, each branch represents an outcome of the test, and each leaf node represents a class label or a numerical value.\n",
    "        * **Optimization (Greedy Approach):** At each step, the tree is split based on the feature and threshold that best reduces impurity (for classification, e.g., Gini impurity or Entropy; for regression, e.g., MSE).\n",
    "            * **Gini Impurity (for Classification):**\n",
    "                $$Gini = 1 - \\sum_{k=1}^{C} p_k^2$$\n",
    "                (where $p_k$ is proportion of class $k$).\n",
    "            * **Entropy (for Classification):**\n",
    "                $$Entropy = -\\sum_{k=1}^{C} p_k \\log_2(p_k)$$\n",
    "                Information Gain is the reduction in entropy.\n",
    "        * **Visualization:** A flowchart-like tree structure.\n",
    "    * **Ensemble Models (Random Forest, Boosting):**\n",
    "        * **Concept:** Combining multiple \"weak learners\" to form a stronger, more robust model.\n",
    "        * **Random Forest:** Builds multiple decision trees, each trained on a random subset of data and features. Predictions are averaged (regression) or majority voted (classification). Reduces variance (overfitting).\n",
    "        * **Boosting (e.g., Gradient Boosting, AdaBoost, XGBoost):** Sequentially builds models, where each new model tries to correct the errors of the previous ones. Focuses on reducing bias.\n",
    "            * **Gradient Boosting (Conceptual):** Fits new models to the *residuals* (errors) of the previous models. The final prediction is a sum of the predictions of all individual models. This implicitly optimizes a loss function using gradient descent-like steps in function space.\n",
    "    * **Support Vector Machines (SVMs):**\n",
    "        * **Concept:** Finds the optimal hyperplane that maximally separates data points of different classes.\n",
    "        * **Mathematical Core:** Involves solving a quadratic programming problem to find the optimal hyperplane defined by $w \\cdot x - b = 0$. The goal is to maximize the margin ($2/||w||$) between the separating hyperplane and the closest data points (support vectors).\n",
    "        * **Soft Margin SVM:** Introduces slack variables ($\\xi_i$) to allow for some misclassifications (for non-linearly separable data or noise).\n",
    "            * **Objective (primal form):**\n",
    "                $$\\min_{w,b,\\xi} \\frac{1}{2} ||w||^2 + C \\sum_{i=1}^{m} \\xi_i$$\n",
    "                subject to $y_i (w \\cdot x_i - b) \\ge 1 - \\xi_i$ and $\\xi_i \\ge 0$.\n",
    "                * $C$ is a regularization parameter.\n",
    "        * **Kernel Trick:** Allows SVMs to find non-linear decision boundaries by implicitly mapping data into higher-dimensional feature spaces (e.g., polynomial, RBF kernels).\n",
    "        * **Visualization:** A separating line/plane with a \"margin\" around it, indicating the support vectors. For kernel SVM, the decision boundary can be curved.\n",
    "    * **Machine Learning Pipelines:** A structured workflow, automating steps like data preprocessing, model training, and evaluation. Ensures consistency and reproducibility.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675faab6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3dedaebc",
   "metadata": {},
   "source": [
    "# IV) Machine Learning Models with Scikit-learn (Unsupervised Learning):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd0ca09",
   "metadata": {},
   "source": [
    "* **Machine Learning Models with Scikit-learn (Unsupervised Learning):**\n",
    "    * **Concept:** Discovering patterns and structure in data without explicit labels.\n",
    "    * **Clustering (e.g., K-Means):** Grouping similar data points.\n",
    "        * **K-Means Optimization:** Minimizes the **Within-Cluster Sum of Squares (WCSS)** or inertia:\n",
    "            $$WCSS = \\sum_{j=1}^{k} \\sum_{x \\in C_j} ||x - \\mu_j||^2$$\n",
    "            (where $C_j$ is cluster $j$, $\\mu_j$ is its centroid).\n",
    "            * **Algorithm (Iterative Optimization):**\n",
    "                1.  Initialize K centroids.\n",
    "                2.  Assign each data point to the nearest centroid (minimizing Euclidean distance: $d(x,y) = \\sqrt{\\sum (x_i - y_i)^2}$).\n",
    "                3.  Update centroids to be the mean of the assigned points.\n",
    "                4.  Repeat steps 2-3 until centroids no longer change significantly.\n",
    "        * **Visualization:** Data points colored by cluster, with centroids marked.\n",
    "    * **Distance Metrics:** How similarity/dissimilarity is measured.\n",
    "        * **Euclidean Distance:**\n",
    "            $$d(\\mathbf{p}, \\mathbf{q}) = \\sqrt{\\sum_{i=1}^{n} (q_i - p_i)^2}$$\n",
    "        * **Manhattan Distance (L1 norm):**\n",
    "            $$d(\\mathbf{p}, \\mathbf{q}) = \\sum_{i=1}^{n} |q_i - p_i|$$\n",
    "    * **Principal Component Analysis (PCA):**\n",
    "        * **Concept:** A dimensionality reduction technique that transforms data into a new coordinate system, where the axes (principal components) capture the most variance.\n",
    "        * **Mathematical Core:** Involves **eigen-decomposition** of the covariance matrix of the data. The principal components are the eigenvectors corresponding to the largest eigenvalues.\n",
    "            * **Covariance Matrix ($\\Sigma$):** Measures how much two variables change together. For a matrix $X$,\n",
    "                $$\\Sigma = \\frac{1}{m} (X - \\bar{X})^T (X - \\bar{X})$$\n",
    "            * **Eigenvectors & Eigenvalues:** For a matrix $A$, if $Av = \\lambda v$, then $v$ is an eigenvector and $\\lambda$ is its eigenvalue.\n",
    "        * **Optimization:** Finding the directions of maximum variance.\n",
    "        * **Visualization:** Reducing a 3D scatter plot to a 2D plot while retaining the most \"spread\" of the data.\n",
    "    * **Gaussian Mixture Models (GMM):**\n",
    "        * **Concept:** Assumes data points are generated from a mixture of several Gaussian distributions.\n",
    "        * **Optimization:** Uses the **Expectation-Maximization (EM) algorithm**. It's an iterative optimization strategy for models with latent (hidden) variables.\n",
    "            * **E-step (Expectation):** Calculate the probability (responsibility) that each data point belongs to each Gaussian component.\n",
    "            * **M-step (Maximization):** Update the parameters (mean, covariance, weight) of each Gaussian component based on the responsibilities.\n",
    "            * Repeat until convergence.\n",
    "        * **Visualization:** Data points with overlapping elliptical contours representing the individual Gaussian distributions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf04e4f8",
   "metadata": {},
   "source": [
    "# Learning Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a27b8a",
   "metadata": {},
   "source": [
    " \n",
    "\n",
    "#### Foundations of Machine Learning Progression:\n",
    "\n",
    "- **Big Data & Data Preprocessing**  \n",
    "  → Cleaning, transforming, and preparing structured and unstructured data.\n",
    "\n",
    "- **Inferential Statistics**  \n",
    "  → Hypothesis testing, confidence intervals, and distribution-based reasoning.\n",
    "\n",
    "- **Linear & Logistic Regression**  \n",
    "  → Foundational supervised learning models for continuous and binary outcomes.\n",
    "\n",
    "- **Advanced Supervised Machine Learning Models**  \n",
    "  → Decision Trees, Random Forest, Boosting (e.g., XGBoost), Support Vector Machines (SVM).\n",
    "\n",
    "- **Unsupervised Learning Models**  \n",
    "  → Clustering (e.g., K-Means), Principal Component Analysis (PCA), Gaussian Mixture Models (GMM).\n",
    "\n",
    "> **Note**: Roughly **80% of business problems** fall within the above ML categories.\n",
    "\n",
    "---\n",
    "\n",
    "#### Emerging Trends\n",
    "\n",
    "- **Natural Language Processing (NLP) and Generative AI (GenAI)**  \n",
    "  → NLP tasks like sentiment analysis, summarization, and document classification are now powered by **Large Language Models (LLMs)** such as ChatGPT, BERT, Claude, and Gemini.\n",
    "\n",
    "- **Neural Networks and Deep Learning**  \n",
    "  → Learning representations from images, text, time series, and audio using multi-layered networks (e.g., CNNs, RNNs, Transformers).\n",
    "\n",
    ">  Over the last year, **LLMs and Deep Learning** have become essential because they can **solve traditional ML problems with minimal expert intervention**, enabling rapid automation and advanced human-like understanding.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7584317",
   "metadata": {},
   "source": [
    "## 📚 Machine Learning Classifications\n",
    "\n",
    "### 1. **Supervised Learning** ✅  \n",
    "- **Definition**: The model learns from a labeled dataset—input-output pairs are given.  \n",
    "- **Goal**: Predict an output (label) from input features.  \n",
    "- **Examples**:  \n",
    "  - **Classification**: Spam detection, disease diagnosis  \n",
    "  - **Regression**: House price prediction, energy demand forecasting  \n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Unsupervised Learning** ✅  \n",
    "- **Definition**: The model tries to find hidden patterns or structure in data without labels.  \n",
    "- **Goal**: Group or reduce the data based on similarities.  \n",
    "- **Examples**:  \n",
    "  - **Clustering**: Customer segmentation  \n",
    "  - **Dimensionality Reduction**: PCA for visualization or noise reduction  \n",
    "\n",
    "---\n",
    "\n",
    "### 3. **Reinforcement Learning** 🔁  \n",
    "- **Definition**: An **agent** interacts with an **environment**, learns from **rewards and penalties**, and aims to maximize long-term reward.  \n",
    "- **Goal**: Learn a policy or strategy that yields the best outcome over time.  \n",
    "- **Examples**:  \n",
    "  - Teaching a **dog or child** via **reward/punishment**  \n",
    "  - Game playing (e.g., AlphaGo), robotics, autonomous driving  \n",
    "- **Key Idea**: **Live, feedback-driven learning**, often modeled with **Markov Decision Processes (MDPs)**\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c8be61",
   "metadata": {},
   "source": [
    "##  This Week: Advanced Machine Learning Models\n",
    "\n",
    "This week's focus is on models designed to handle **complex, unstructured, and sequential data**.\n",
    "\n",
    "###  Topics Covered:\n",
    "1. **Natural Language Processing (NLP) Tasks**  \n",
    "2. **Time Series Forecasting**  \n",
    "3. **Neural Networks and Deep Learning Frameworks**\n",
    "\n",
    "---\n",
    "\n",
    "##  How Are These Tasks Different from Traditional Models?\n",
    "\n",
    "| Area | Advanced ML (This Week) | Traditional ML (Linear, RF, SVM, Clustering, PCA) |\n",
    "|------|-------------------------|---------------------------------------------------|\n",
    "| **Data Structure** | Complex, unstructured data (text, sequences, images, audio, video) | Structured/tabular data |\n",
    "| **Temporal Context** | Models like RNNs and LSTMs capture time dependencies (e.g., today vs. tomorrow) | No inherent time modeling |\n",
    "| **Text & Language** | Designed for understanding and generating human language | Limited or no support for text semantics |\n",
    "| **Interpretability** | Often considered black-box models | Linear models & trees are more transparent |\n",
    "| **Applications** | NLP, forecasting, speech recognition, image recognition, chatbots | Classification, regression, clustering, anomaly detection |\n",
    "\n",
    "---\n",
    "\n",
    "###  Summary\n",
    "\n",
    "- This week's models are built for **unstructured or sequential data**—such as **text, images, audio, and video**.\n",
    "- These models are **more complex**, but thanks to **modern tools and libraries**, we can build and train them more easily.\n",
    "- **Foundational understanding** of these models is essential for advancing into cutting-edge AI applications.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34bc056c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3a0da2e9",
   "metadata": {},
   "source": [
    "###  Traditional NLP vs. Modern NLP (LLM-Based)\n",
    "\n",
    "---\n",
    "\n",
    "#### Traditional NLP — Tasks & Algorithms\n",
    "\n",
    "**Common Tasks:**\n",
    "1. **Text Classification** – e.g., spam detection, sentiment analysis (positive/negative)\n",
    "2. **Named Entity Recognition (NER)** – Identify people, organizations, companies (e.g., extracting signers' names, addresses, and signatures from 100+ page legal documents)\n",
    "3. **Machine Translation** – e.g., English to French\n",
    "4. **Question Answering**\n",
    "5. **Text Summarization**\n",
    "6. **Speech Recognition**\n",
    "\n",
    "**Common Algorithms:**\n",
    "- Naive Bayes  \n",
    "- Logistic Regression  \n",
    "- Support Vector Machines (SVM)  \n",
    "- Decision Trees (DT)  \n",
    "\n",
    "These models often require manual feature engineering (e.g., TF-IDF, Bag of Words).\n",
    "\n",
    "---\n",
    "\n",
    "#### Modern NLP — LLM-Based Pipeline\n",
    "\n",
    "**Key Characteristics:**\n",
    "- Deep learning-based models\n",
    "- Context-aware\n",
    "- End-to-end learning with minimal feature engineering\n",
    "\n",
    "**Backbone Models:**\n",
    "- RNNs, LSTMs\n",
    "- Attention Mechanism (foundation of Transformers)\n",
    "\n",
    "**Popular Transformer-Based LLMs:**\n",
    "- **BERT** (Google)\n",
    "- **T5** (Google)\n",
    "- **GPT-2 / GPT-4** (OpenAI)\n",
    "- **Claude** (Anthropic)\n",
    "- **Gemini** (Google DeepMind)\n",
    "- **LLaMA** (Meta)\n",
    "\n",
    "These models dominate current NLP tasks due to their ability to learn contextual meaning across sentences and documents.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a2e0ebe",
   "metadata": {},
   "source": [
    "## Traditional NLP Pipeline – Core Task Process\n",
    "\n",
    "In Traditional NLP, the journey from raw text to machine-understandable format involves two major phases:\n",
    "\n",
    "---\n",
    "\n",
    "###  I. Text Preprocessing Steps\n",
    "\n",
    "We are working with **documents, statements, phrases, and words**. These raw forms must be cleaned and structured for modeling.\n",
    "\n",
    "#### 1. Tokenization\n",
    "- **Definition**: Splitting text into individual units (words or tokens).\n",
    "- **Example**:  \n",
    "  `\"Flatirons School is great but it is intensive\"`  \n",
    "  ⟶ `[\"Flatirons\", \"School\", \"is\", \"great\", \"but\", \"it\", \"is\", \"intensive\"]`\n",
    "- Advanced tokenizers can preserve context, e.g., “New York” as one token.\n",
    "\n",
    "#### 2. Stopword Removal\n",
    "- Removes common words like *“the”*, *“is”*, *“and”*, which add little semantic value.\n",
    "- Purpose: Reduce noise in modeling.\n",
    "\n",
    "#### 3. Stemming and Lemmatization\n",
    "- **Stemming**: Crude way to chop off word endings (e.g., \"running\" → \"run\").\n",
    "- **Lemmatization**: Converts words to their dictionary form (e.g., \"better\" → \"good\").\n",
    "\n",
    "#### 4. Normalization\n",
    "- Lowercasing, removing punctuation, special characters, extra spaces.\n",
    "- Example: `\"HELLO!! World.\"` → `\"hello world\"`\n",
    "\n",
    "> **Note:** Preprocessing is the heavy lifting of NLP. Clean text = better models.\n",
    "\n",
    "---\n",
    "\n",
    "###  II. Text Representation – Converting Text to Numbers\n",
    "\n",
    "Machine learning models require **numerical input**, so we must transform text into vectors.\n",
    "\n",
    "#### 1. Bag of Words (BoW)\n",
    "- Represents documents by word counts.\n",
    "- Ignores order/context.\n",
    "- Example:\n",
    "  - Doc 1: \"NLP is great\"\n",
    "  - Doc 2: \"NLP is hard\"\n",
    "  - Vocabulary: [“NLP”, “is”, “great”, “hard”]\n",
    "  - Vectorized: `[1, 1, 1, 0]`, `[1, 1, 0, 1]`\n",
    "\n",
    "#### 2. TF-IDF (Term Frequency-Inverse Document Frequency)\n",
    "- Weighs words based on frequency in a document vs. across all documents.\n",
    "- Reduces weight of common words and highlights rare but important ones.\n",
    "\n",
    "#### 3. Word Embeddings (Dense Vector Representations)\n",
    "- Captures **semantic meaning** by mapping words to continuous vector spaces.\n",
    "- Examples:\n",
    "  - **Word2Vec**\n",
    "  - **GloVe**\n",
    "  - **FastText**\n",
    "  - **Transformer-based contextual embeddings** (BERT, GPT)\n",
    "\n",
    "> 🎯 Goal: Represent documents and words in a form suitable for ML tasks like classification, summarization, NER, and translation.\n",
    "\n",
    "---\n",
    "\n",
    "###  Pretrained Models & Pipelines\n",
    "\n",
    "- **Libraries**: spaCy, NLTK, Gensim, Hugging Face Transformers\n",
    "- **Pretrained Models**: BERT, RoBERTa, GPT can be fine-tuned for custom NLP tasks.\n",
    "- If a pretrained model performs well, you **don’t need to retrain** from scratch.\n",
    "- For domain-specific applications (e.g., legal, healthcare), further fine-tuning on new corpora is often beneficial.\n",
    "\n",
    "---\n",
    "\n",
    "### Summary\n",
    "\n",
    "Text preprocessing and representation form the **backbone of traditional NLP pipelines**. These steps turn messy, human-readable language into structured numerical features, ready for machine learning.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0cdd439",
   "metadata": {},
   "source": [
    "\n",
    "### V). The Frontier: Sequence, Structure, and Deeper Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59626862",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "### V). The Frontier: Sequence, Structure, and Deeper Learning\n",
    "\n",
    "* **Natural Language Processing (NLP):**\n",
    "    * **Concept:** Enabling computers to understand, interpret, and generate human language.\n",
    "    * **Mathematical Relevance:** Relies heavily on probability, linear algebra (e.g., word embeddings as vectors), and optimization techniques for training language models.\n",
    "        * **Word Embeddings (e.g., Word2Vec, GloVe):** Represent words as dense vectors in a continuous vector space, where semantic similarity is captured by vector proximity (e.g., using cosine similarity).\n",
    "            $$\\text{cosine similarity}(A, B) = \\frac{A \\cdot B}{||A|| ||B||}$$\n",
    "        * **Bag-of-Words (BoW):** Simple vector representation based on word counts.\n",
    "        * **TF-IDF (Term Frequency-Inverse Document Frequency):** Weighs words by their importance.\n",
    "            $$TF(t,d) = \\frac{\\text{Number of times term t appears in document d}}{\\text{Total number of terms in document d}}$$           $$IDF(t,D) = \\log \\frac{\\text{Total number of documents}}{\\text{Number of documents with term t}}$$           $$TFIDF(t,d,D) = TF(t,d) \\times IDF(t,D)$$\n",
    "    * **Visualization:** Word clouds, vector space plots (2D/3D) showing word relationships.\n",
    "\n",
    "* **Exploring Time Series Data:**\n",
    "    * **Concept:** Analyzing data points collected over time where order matters.\n",
    "    * **Mathematical Relevance:** Statistical models (ARIMA, Exponential Smoothing), Fourier transforms for seasonality, and specialized deep learning architectures (RNNs, LSTMs).\n",
    "        * **Autoregressive (AR) Models:** A variable's current value depends linearly on its previous values.\n",
    "            $$Y_t = c + \\phi_1 Y_{t-1} + \\dots + \\phi_p Y_{t-p} + \\epsilon_t$$\n",
    "        * **Moving Average (MA) Models:** A variable's current value depends on past forecast errors.\n",
    "            $$Y_t = \\mu + \\epsilon_t + \\theta_1 \\epsilon_{t-1} + \\dots + \\theta_q \\epsilon_{t-q}$$\n",
    "    * **Visualization:** Line graphs showing trends, seasonality, and cycles; autocorrelation plots (ACF) and partial autocorrelation plots (PACF) to identify patterns.\n",
    "\n",
    "* **Fundamentals of Neural Networks:**\n",
    "    * **Concept:** Inspired by the human brain, composed of interconnected \"neurons\" organized in layers.\n",
    "    * **Mathematical Core:**\n",
    "        * **Neuron (Perceptron) Activation:**\n",
    "            1.  **Weighted Sum:** $z = \\sum_{j=1}^{n} w_j x_j + b$ (vector form: $z = w^T x + b$)\n",
    "            2.  **Activation Function:** $a = f(z)$\n",
    "            * **Common Activation Functions:**\n",
    "                * **Sigmoid:**\n",
    "                    $$\\sigma(z) = \\frac{1}{1 + e^{-z}}$$\n",
    "                    (squashes output to 0-1)\n",
    "                * **ReLU (Rectified Linear Unit):**\n",
    "                    $$f(z) = \\max(0, z)$$\n",
    "                    (most common in hidden layers)\n",
    "                * **Softmax:** Used in the output layer for multi-class classification:\n",
    "                    $$P(y=k|\\mathbf{x}) = \\frac{e^{z_k}}{\\sum_{j=1}^{K} e^{z_j}}$$\n",
    "        * **Loss Function:** Quantifies the difference between predicted and actual outputs (e.g., MSE for regression, Cross-Entropy for classification).\n",
    "    * **Visualization:** Network diagrams with nodes and weighted connections. Input, hidden, and output layers.\n",
    "\n",
    "* **Deep Learning with Neural Networks:**\n",
    "    * **Concept:** Neural networks with many (deep) hidden layers, capable of learning hierarchical representations.\n",
    "    * **Why it Matters:** Achieved state-of-the-art results due to their ability to learn complex, non-linear patterns directly from raw data.\n",
    "    * **Optimization (Key to \"Learning\"):**\n",
    "        * **Gradient Descent (and its variants: SGD, Mini-batch GD, Adam, RMSprop):** The primary optimization algorithm. It iteratively updates the weights and biases by moving in the direction opposite to the gradient of the loss function.\n",
    "            * **Weight Update Rule:**\n",
    "                $$W := W - \\alpha \\frac{\\partial L}{\\partial W}$$\n",
    "            * **Bias Update Rule:**\n",
    "                $$b := b - \\alpha \\frac{\\partial L}{\\partial b}$$\n",
    "            * $\\alpha$ is the learning rate.\n",
    "        * **Backpropagation:** The algorithm for efficiently calculating the gradients ($\\frac{\\partial L}{\\partial W}$, $\\frac{\\partial L}{\\partial b}$) of the loss function with respect to each weight and bias in the network. It uses the chain rule of calculus to propagate errors backward through the network.\n",
    "            * **Chain Rule:**\n",
    "                $$\\frac{dy}{dx} = \\frac{dy}{du} \\cdot \\frac{du}{dx}$$\n",
    "            * **Conceptual:** Calculates how much each weight/bias contributes to the overall error.\n",
    "        * **Epochs & Batches:** Training occurs over multiple epochs (full passes through the dataset). Data is processed in batches to manage memory and provide smoother gradient estimates.\n",
    "    * **Visualization:** More complex network diagrams. Loss function convergence plots over epochs.\n",
    "\n",
    "* **Multilayer Perceptrons (MLPs):**\n",
    "    * **Concept:** A foundational type of feedforward neural network with one or more hidden layers. Each neuron in a layer is connected to every neuron in the previous and subsequent layers.\n",
    "    * **Mathematical Flow:** Input -> Weighted Sums -> Activations -> Weighted Sums (next layer) -> Activations -> Output Layer. This entire process is differentiated using backpropagation to find optimal weights.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419f7820",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
