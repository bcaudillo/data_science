{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44cc783c",
   "metadata": {},
   "source": [
    "## Part 1: NLP - Text Processing and Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636536eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Imports for NLP\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e2ad6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load dataset (replace this with your real dataset)\n",
    "data = pd.DataFrame({\n",
    "    'category': ['support', 'sales', 'billing', 'technical', 'support'],\n",
    "    'text': [\n",
    "        \"My payment failed, please assist.\",\n",
    "        \"I would like to inquire about product pricing.\",\n",
    "        \"I have a billing issue with last monthâ€™s invoice.\",\n",
    "        \"My router stopped working suddenly.\",\n",
    "        \"How do I reset my password?\"\n",
    "    ]\n",
    "})\n",
    "\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2659ad30",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO: Create full preprocessing function\n",
    "def preprocess_text(text):\n",
    "    # 1. Lowercase\n",
    "    # 2. Remove punctuation\n",
    "    # 3. Tokenize\n",
    "    # 4. Remove stopwords\n",
    "    # 5. (Optional) Lemmatize\n",
    "    return text  # <-- replace with cleaned text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d69ab29",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Apply preprocessing function (once you fill it in above)\n",
    "data['clean_text'] = data['text'].apply(preprocess_text)\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7804cbfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO: Token Frequency Analysis\n",
    "# Use Counter to count most common words after cleaning\n",
    "# Output top 10 words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec579e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO: N-gram Analysis\n",
    "# Use CountVectorizer to extract bigrams and trigrams\n",
    "# Output most common bigrams and trigrams\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e2675b7",
   "metadata": {},
   "source": [
    "## Part 2: Time Series Forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8426297",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Imports for time series\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "\n",
    "# TODO: Load real S&P 500 or financial dataset here\n",
    "# For now we simulate data\n",
    "dates = pd.date_range(start='2015-01-01', periods=100, freq='D')\n",
    "values = np.random.randn(100).cumsum() + 1000\n",
    "ts_data = pd.Series(values, index=dates)\n",
    "ts_data.plot(title='Simulated Time Series Data')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af1f0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO: Perform ADF test for stationarity\n",
    "# Use adfuller() function and print ADF statistic and p-value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3baffd0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO: Apply transformations if data is not stationary\n",
    "# Example: Differencing or log transform\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ceef3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO: Decompose time series using seasonal_decompose()\n",
    "# Plot decomposition components\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322a4cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO: Plot ACF and PACF to examine autocorrelations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddefe332",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO: Fit ARIMA or Exponential Smoothing model to forecast\n",
    "# Print forecast values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b5f309",
   "metadata": {},
   "source": [
    "## Part 3: Neural Networks (Classification & Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd855ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "\n",
    "from sklearn.datasets import load_digits, load_diabetes\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a35895",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Classification Dataset (Digits)\n",
    "digits = load_digits()\n",
    "X_digits, y_digits = digits.data, digits.target\n",
    "\n",
    "X_train_c, X_test_c, y_train_c, y_test_c = train_test_split(X_digits, y_digits, test_size=0.2, random_state=42)\n",
    "scaler_c = StandardScaler()\n",
    "X_train_c = scaler_c.fit_transform(X_train_c)\n",
    "X_test_c = scaler_c.transform(X_test_c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3437dfec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO: Build Classification Neural Network (TensorFlow)\n",
    "# Use Sequential model, at least 2 hidden layers, softmax output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1a8d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO: Compile, train, and evaluate classification model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b368dd16",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Regression Dataset (Diabetes)\n",
    "diabetes = load_diabetes()\n",
    "X_reg, y_reg = diabetes.data, diabetes.target\n",
    "\n",
    "X_train_r, X_test_r, y_train_r, y_test_r = train_test_split(X_reg, y_reg, test_size=0.2, random_state=42)\n",
    "scaler_r = StandardScaler()\n",
    "X_train_r = scaler_r.fit_transform(X_train_r)\n",
    "X_test_r = scaler_r.transform(X_test_r)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1901ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO: Build Regression Neural Network (TensorFlow)\n",
    "# Output layer should have 1 unit (for regression)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee630fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO: Compile, train, and evaluate regression model\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
