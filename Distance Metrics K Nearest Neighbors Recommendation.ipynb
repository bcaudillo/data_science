{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Key Concepts for Week2 -Introduction to Machine Learning \n",
    "\n",
    "### 1) Distance Metrics\n",
    "### 2) K-Nearest Neighbors (KNN)\n",
    "### 3) Recommendation Systems\n",
    "### 4) Principal Component Analysis (PCA)\n",
    "### 5) Clustering\n",
    "### 6) Gaussian Mixture Modeling (GMM)\n",
    "### 7) Market Segmentation using Clustering Models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VkukeKeNxNVh"
   },
   "source": [
    "## Conceptual: Distance Metrics\n",
    "\n",
    "### Title: Understanding Distances: The Foundation of Data Relationships\n",
    "\n",
    "### 1. Introduction: Why Measure Distance?\n",
    "\n",
    "* **Concept:** In machine learning, especially for tasks like classification, clustering, and recommendation, understanding how \"similar\" or \"dissimilar\" data points are is crucial. Distance metrics provide a quantitative way to measure this.\n",
    "* **Analogy:** Think of measuring physical distance between two cities on a map. In data, we're measuring the \"distance\" between data points in a multi-dimensional space.\n",
    "\n",
    "\n",
    "## Key Distance Metrics\n",
    "\n",
    "To understand how similar or dissimilar data points are, we use **distance metrics**. These are fundamental in many machine learning algorithms like K-Nearest Neighbors, Clustering, and Recommendation Systems.\n",
    "\n",
    "---\n",
    "\n",
    "### Euclidean Distance\n",
    "\n",
    "* **Concept:** This is the most common \"straight-line\" distance between two points in a Euclidean space. Think of it as measuring the length of the hypotenuse of a right triangle formed by the coordinate differences.\n",
    "* **Formula (for 2 points $p=(p_1, p_2)$ and $q=(q_1, q_2)$):**\n",
    "    $$D(p,q) = \\sqrt{(p_1-q_1)^2 + (p_2-q_2)^2}$$\n",
    "* **General Formula (for n-dimensions):**\n",
    "    $$D(p,q) = \\sqrt{\\sum_{i=1}^{n}(p_i - q_i)^2}$$\n",
    "* **Use Cases:** K-Nearest Neighbors, K-Means Clustering, any application where the \"straight-line\" distance makes intuitive sense.\n",
    "\n",
    "---\n",
    "\n",
    "### Manhattan Distance (L1 Norm / City Block Distance)\n",
    "\n",
    "* **Concept:** This metric calculates the sum of the absolute differences of the Cartesian coordinates. Imagine navigating a city grid where you can only move horizontally or vertically â€“ you can't cut diagonally through blocks.\n",
    "* **Formula (for 2 points $p=(p_1, p_2)$ and $q=(q_1, q_2)$):**\n",
    "    $$D(p,q) = |p_1-q_1| + |p_2-q_2|$$\n",
    "* **General Formula (for n-dimensions):**\n",
    "    $$D(p,q) = \\sum_{i=1}^{n}|p_i - q_i|$$\n",
    "* **Use Cases:** When features are not necessarily correlated, or when outliers should have less impact as it measures absolute differences rather than squared differences.\n",
    "\n",
    "---\n",
    "\n",
    "### Minkowski Distance\n",
    "\n",
    "* **Concept:** The Minkowski distance is a generalization of both Euclidean and Manhattan distances. It introduces a parameter 'r' that allows you to vary the \"path\" taken between points.\n",
    "* **Formula:**\n",
    "    $$D(p,q) = \\left(\\sum_{i=1}^{n}|p_i - q_i|^r\\right)^{1/r}$$\n",
    "* **Note:**\n",
    "    * When $r=1$, it becomes the **Manhattan Distance**.\n",
    "    * When $r=2$, it becomes the **Euclidean Distance**.\n",
    "* **Use Cases:** Provides flexibility when you need to experiment with different distance calculation methods, or when you have a specific reason to use an 'r' value other than 1 or 2.\n",
    "\n",
    "\n",
    "\n",
    "### Cosine Similarity (and Distance)\n",
    "\n",
    "* **Concept:** Unlike the previous metrics that focus on the magnitude of difference, Cosine Similarity measures the **cosine of the angle between two non-zero vectors**. It indicates how similar the *orientation* of two vectors is, regardless of their magnitude. A higher cosine similarity (closer to 1) means a smaller angle and more similar orientation.\n",
    "* **Formula (Similarity):**\n",
    "    $$Similarity = \\frac{A \\cdot B}{||A|| \\cdot ||B||} = \\frac{\\sum A_i B_i}{\\sqrt{\\sum A_i^2} \\sqrt{\\sum B_i^2}}$$\n",
    "    (Where $A \\cdot B$ is the dot product, and $||A||$ and $||B||$ are the magnitudes/L2 norms of vectors A and B, respectively).\n",
    "* **Cosine Distance:** Often derived from similarity:\n",
    "    $$Distance = 1 - Similarity$$\n",
    "* **Use Cases:** Text analysis (e.g., finding similar documents or comparing word embeddings), recommendation systems (e.g., finding users with similar preferences or items with similar content).\n",
    "\n",
    "---\n",
    "\n",
    "### 3. How it Relates\n",
    "\n",
    "* **K-Nearest Neighbors (KNN):** Distance metrics are *fundamental* to KNN to find the 'nearest' neighbors.\n",
    "* **Clustering:** Algorithms like K-Means use distance metrics to group similar data points together.\n",
    "* **Recommendation Systems:** Often use distance/similarity to find similar users or items.\n",
    "\n",
    "### 4. Example Scenario\n",
    "\n",
    "* **Scenario:** Imagine a dataset of customers with features like age, income, and spending score. We want to find customers who are \"similar\" to each other.\n",
    "* **Application:** We could use Euclidean distance to group customers with similar numerical profiles, or Cosine similarity if we were dealing with their purchasing patterns (e.g., product categories they buy).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "DSxKJU6hxNVm"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chala\\anaconda3\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "# Python Snippet\n",
    "\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import euclidean, cityblock, cosine\n",
    "\n",
    "# Example data points\n",
    "point1 = np.array([1, 2, 3])\n",
    "point2 = np.array([4, 5, 6])\n",
    "\n",
    "# Euclidean Distance\n",
    "# distance_euclidean = euclidean(point1, point2)\n",
    "# print(f\"Euclidean Distance: {distance_euclidean}\")\n",
    "\n",
    "# Manhattan Distance\n",
    "# distance_manhattan = cityblock(point1, point2)\n",
    "# print(f\"Manhattan Distance: {distance_manhattan}\")\n",
    "\n",
    "# Cosine Similarity and Distance\n",
    "# similarity_cosine = 1 - cosine(point1, point2) # cosine returns distance, so 1 - distance for similarity\n",
    "# print(f\"Cosine Similarity: {similarity_cosine}\")\n",
    "# distance_cosine = cosine(point1, point2)\n",
    "# print(f\"Cosine Distance: {distance_cosine}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tPB2OPKXxNVn"
   },
   "source": [
    "### 6. Discussion Points\n",
    "\n",
    "* When would you choose Euclidean over Manhattan distance?\n",
    "* How does the scale of features affect distance metrics? (Leads to feature scaling discussion).\n",
    "* What are the advantages and disadvantages of Cosine Similarity?\n",
    "* Are there other distance metrics not covered here (e.g., Hamming distance for categorical data)?\n",
    "\n",
    "---\n",
    "\n",
    "## Conceptual: K-Nearest Neighbors (KNN)\n",
    "\n",
    "### Title: K-Nearest Neighbors: Simple, Yet Powerful Classification and Regression\n",
    "\n",
    "### 1. Introduction: Learning from Your Neighbors\n",
    "\n",
    "* **Concept:** KNN is a non-parametric, lazy learning algorithm used for both classification and regression. The core idea is to classify or predict based on the majority class or average value of its 'K' nearest neighbors in the feature space.\n",
    "* **Analogy:** \"Tell me who your friends are, and I'll tell you who you are.\" If most of your closest friends like rock music, you probably like rock music too.\n",
    "\n",
    "### 2. How KNN Works (for Classification)\n",
    "\n",
    "* **Training Phase:** KNN has no explicit training phase (hence \"lazy learner\"). It just stores the entire training dataset.\n",
    "* **Prediction Phase:**\n",
    "    1.  For a new, unseen data point, calculate its distance to *all* training data points using a chosen distance metric (e.g., Euclidean).\n",
    "    2.  Identify the 'K' data points in the training set that are closest to the new point (its nearest neighbors).\n",
    "    3.  For classification, assign the new point the class label that is most frequent among its K neighbors (majority vote).\n",
    "    4.  For regression, assign the new point the average (or weighted average) of the target values of its K neighbors.\n",
    "\n",
    "### 3. Key Hyperparameters and Considerations\n",
    "\n",
    "* **K (Number of Neighbors):**\n",
    "    * **Concept:** The most crucial parameter.\n",
    "    * **Impact:**\n",
    "        * Small K: Can be sensitive to noise/outliers, may lead to overfitting.\n",
    "        * Large K: Smoother decision boundaries, reduces variance, but might miss local patterns and lead to underfitting.\n",
    "    * **Selection:** Often determined through cross-validation.\n",
    "* **Distance Metric:** Choice depends on the nature of data (as discussed in Distance Metrics notebook).\n",
    "* **Feature Scaling:** *Crucial*. KNN is highly sensitive to the scale of features because distance calculations are affected by it. Features with larger ranges will dominate the distance calculation.\n",
    "* **Computational Cost:** Can be high for large datasets during prediction, as it requires calculating distances to all training points.\n",
    "\n",
    "### 4. How it Relates\n",
    "\n",
    "* **Distance Metrics:** Directly uses distance metrics to find neighbors.\n",
    "* **Curse of Dimensionality:** KNN's performance degrades in high-dimensional spaces because distances become less meaningful (all points tend to be \"far\" from each other).\n",
    "\n",
    "### 5. Example Scenario\n",
    "\n",
    "* **Scenario:** Classifying emails as \"Spam\" or \"Not Spam\" based on features like word count, presence of certain keywords, sender reputation.\n",
    "* **Application:** When a new email arrives, KNN finds its K nearest emails from the training set (already labeled as Spam or Not Spam) and assigns the new email the majority label.\n",
    "\n",
    "### 6. Conceptual Code Snippets (Python)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "wXT5E7eTxNVo"
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Sample data (conceptual)\n",
    "# X = pd.DataFrame({'Feature1': [1,2,3,4,5,6], 'Feature2': [10,20,15,25,30,22]})\n",
    "# y_classification = pd.Series([0,0,1,1,0,1]) # Example labels\n",
    "# y_regression = pd.Series([100,120,110,130,150,140]) # Example values\n",
    "\n",
    "# 1. Feature Scaling (Crucial for KNN)\n",
    "# scaler = StandardScaler()\n",
    "# X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# 2. Split data\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_classification, test_size=0.2, random_state=42)\n",
    "\n",
    "# 3. K-Nearest Neighbors Classifier\n",
    "# knn_classifier = KNeighborsClassifier(n_neighbors=3, metric='euclidean')\n",
    "# knn_classifier.fit(X_train, y_train)\n",
    "# y_pred_classification = knn_classifier.predict(X_test)\n",
    "# accuracy = accuracy_score(y_test, y_pred_classification)\n",
    "# print(f\"KNN Classification Accuracy: {accuracy}\")\n",
    "\n",
    "# 4. K-Nearest Neighbors Regressor\n",
    "# X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(X_scaled, y_regression, test_size=0.2, random_state=42)\n",
    "# knn_regressor = KNeighborsRegressor(n_neighbors=3)\n",
    "# knn_regressor.fit(X_train_reg, y_train_reg)\n",
    "# y_pred_regression = knn_regressor.predict(X_test_reg)\n",
    "# mse = mean_squared_error(y_test_reg, y_pred_regression)\n",
    "# print(f\"KNN Regression MSE: {mse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VM4KKyp7xNVo"
   },
   "source": [
    "### 7. Discussion Points\n",
    "\n",
    "* What are the main advantages and disadvantages of KNN?\n",
    "* How does the \"curse of dimensionality\" affect KNN?\n",
    "* When would you prefer KNN over more complex models like SVMs or Neural Networks?\n",
    "* How can we choose the optimal 'K' value? (Introduce concept of elbow method or cross-validation).\n",
    "\n",
    "---\n",
    "\n",
    "## Conceptual: Recommendation Systems\n",
    "\n",
    "### Title: Recommendation Systems: Guiding Choices in a World of Options\n",
    "\n",
    "### 1. Introduction: Beyond Simple Search\n",
    "\n",
    "* **Concept:** Recommendation systems aim to predict user preferences and suggest items (products, movies, news articles, etc.) that are most likely to be of interest. They are ubiquitous in e-commerce, streaming services, and social media.\n",
    "* **Goal:** Increase user engagement, satisfaction, and sales.\n",
    "\n",
    "### 2. Types of Recommendation Systems\n",
    "\n",
    "* **A. Content-Based Filtering:**\n",
    "    * **Concept:** Recommends items similar to those a user has liked in the past.\n",
    "    * **How it Works:** Builds a profile for each user based on their past interactions (e.g., genre of movies they watched, keywords in articles they read). It then recommends new items whose attributes match the user's profile.\n",
    "    * **Strengths:** Can recommend niche items, no \"cold start\" for new items (if content is available).\n",
    "    * **Weaknesses:** Limited to recommending items similar to what's already known, can't suggest items outside the user's past preferences (\"filter bubble\").\n",
    "    * **Relates to:** Distance Metrics (e.g., Cosine Similarity for item attributes).\n",
    "\n",
    "* **B. Collaborative Filtering:**\n",
    "    * **Concept:** Recommends items based on the preferences of *similar users* or on *item similarity* derived from user ratings.\n",
    "    * **Types:**\n",
    "        * **User-Based Collaborative Filtering:** \"Users who are similar to you also liked...\" Find users with similar tastes, then recommend items those users liked but the current user hasn't seen.\n",
    "        * **Item-Based Collaborative Filtering:** \"People who liked this item also liked...\" Find items similar to those the current user liked, based on how other users rated them.\n",
    "    * **Strengths:** Can discover new and unexpected items, doesn't require explicit content features.\n",
    "    * **Weaknesses:** \"Cold start\" problem for new users (no ratings) or new items (no ratings), sparsity of data (most users rate only a few items).\n",
    "    * **Relates to:** Distance Metrics (e.g., Pearson Correlation for user similarity, Cosine Similarity for item similarity).\n",
    "    * **Example:** Matrix Factorization (e.g., SVD) is a popular technique for collaborative filtering, decomposing the user-item interaction matrix into lower-dimensional latent factors.\n",
    "\n",
    "* **C. Hybrid Approaches:**\n",
    "    * **Concept:** Combine content-based and collaborative filtering to leverage the strengths of both and mitigate their weaknesses.\n",
    "\n",
    "### 3. Key Challenges\n",
    "\n",
    "* **Cold Start Problem:** How to recommend to new users or new items with no interaction history.\n",
    "* **Sparsity:** User-item interaction matrices are often very sparse (few ratings compared to total possibilities).\n",
    "* **Scalability:** Handling millions of users and items.\n",
    "* **Serendipity:** Recommending diverse and unexpected items.\n",
    "* **Explainability:** Why was a certain item recommended?\n",
    "\n",
    "### 4. Example Scenario\n",
    "\n",
    "* **Scenario:** A movie streaming service needs to recommend movies to its users.\n",
    "* **Content-Based:** Recommend sci-fi movies to a user who frequently watches sci-fi movies.\n",
    "* **Collaborative (User-Based):** If user A watched \"The Matrix\" and \"Inception,\" and user B also watched \"The Matrix\" and \"Inception,\" then recommend \"Interstellar\" to user A if user B watched it.\n",
    "* **Collaborative (Item-Based):** If many users who watched \"The Matrix\" also watched \"Blade Runner,\" then recommend \"Blade Runner\" to someone who just watched \"The Matrix.\"\n",
    "\n",
    "### 5. Conceptual Code Snippets (Python)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "kZsLb36qxNVp"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Conceptual: User-Item Interaction Matrix (e.g., ratings)\n",
    "# data = {\n",
    "#     'User_A': [5, 4, 0, 1, 0],\n",
    "#     'User_B': [4, 5, 1, 0, 0],\n",
    "#     'User_C': [0, 1, 5, 4, 0],\n",
    "#     'User_D': [0, 0, 4, 5, 0],\n",
    "#     'User_E': [1, 0, 0, 0, 5]\n",
    "# }\n",
    "# user_item_matrix = pd.DataFrame(data, index=['Item_1', 'Item_2', 'Item_3', 'Item_4', 'Item_5']).T\n",
    "# print(\"User-Item Matrix:\\n\", user_item_matrix)\n",
    "\n",
    "# Conceptual: Calculate User Similarity (e.g., Cosine Similarity)\n",
    "# user_similarity = cosine_similarity(user_item_matrix)\n",
    "# user_similarity_df = pd.DataFrame(user_similarity, index=user_item_matrix.index, columns=user_item_matrix.index)\n",
    "# print(\"\\nUser Similarity Matrix (Cosine):\\n\", user_similarity_df)\n",
    "\n",
    "# Conceptual: Item Similarity (transpose matrix for item-item)\n",
    "# item_similarity = cosine_similarity(user_item_matrix.T)\n",
    "# item_similarity_df = pd.DataFrame(item_similarity, index=user_item_matrix.columns, columns=user_item_matrix.columns)\n",
    "# print(\"\\nItem Similarity Matrix (Cosine):\\n\", item_similarity_df)\n",
    "\n",
    "# Conceptual: Simple recommendation logic (e.g., for user_A, find similar users and recommend items they liked)\n",
    "# For a real system, you'd integrate a library like surprise or build more robust logic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EySt0931xNVp"
   },
   "source": [
    "### 6. Discussion Points\n",
    "\n",
    "* What are the core differences between content-based and collaborative filtering?\n",
    "* How do recommendation systems handle the \"cold start\" problem?\n",
    "* What is the role of matrix factorization in collaborative filtering?\n",
    "* Discuss ethical considerations in recommendation systems (e.g., filter bubbles, bias).\n",
    "\n",
    "---\n",
    "\n",
    "## Conceptual  Principal Component Analysis (PCA)\n",
    "\n",
    "### Title: Principal Component Analysis: Unveiling the Hidden Structure of Data\n",
    "\n",
    "### 1. Introduction: Reducing Complexity, Retaining Information\n",
    "\n",
    "* **Concept:** PCA is a dimensionality reduction technique. It transforms a dataset of possibly correlated variables into a new set of uncorrelated variables called Principal Components (PCs). The goal is to capture as much variance (information) as possible in fewer dimensions.\n",
    "* **Why use it?**\n",
    "    * **Data Compression:** Reduce storage space and computational time.\n",
    "    * **Noise Reduction:** Can filter out redundant information.\n",
    "    * **Visualization:** Project high-dimensional data into 2D or 3D for easier plotting.\n",
    "    * **Feature Engineering:** Create new, uncorrelated features for downstream machine learning models.\n",
    "\n",
    "### 2. How PCA Works (Conceptual Steps)\n",
    "\n",
    "1.  **Standardize the Data:** PCA is sensitive to the scale of features, so data should be scaled (e.g., using `StandardScaler`).\n",
    "2.  **Calculate the Covariance Matrix:** This matrix describes how much the variables change together.\n",
    "3.  **Compute Eigenvectors and Eigenvalues:**\n",
    "    * **Eigenvectors:** These are the principal components. They represent the directions (axes) of maximum variance in the data. They are orthogonal (uncorrelated).\n",
    "    * **Eigenvalues:** Represent the amount of variance explained by each corresponding eigenvector (principal component). A larger eigenvalue means that component captures more variance.\n",
    "4.  **Select Principal Components:** Rank the eigenvectors by their eigenvalues in descending order. Choose the top 'k' eigenvectors that explain a sufficient amount of the total variance (e.g., 95%).\n",
    "5.  **Transform the Data:** Project the original data onto the selected 'k' principal components.\n",
    "\n",
    "### 3. Key Concepts\n",
    "\n",
    "* **Variance Explained:** The proportion of total variance in the dataset explained by each principal component.\n",
    "* **Scree Plot:** A plot of eigenvalues in descending order. Used to determine the \"elbow\" point, suggesting an optimal number of components to retain.\n",
    "* **Loadings:** The coefficients of the original variables in the principal components, indicating how much each original variable contributes to each principal component.\n",
    "\n",
    "### 4. How it Relates\n",
    "\n",
    "* **Clustering & Classification:** Reduced dimensionality can improve the performance and speed of clustering and classification algorithms by removing noise and multicollinearity.\n",
    "* **Data Preprocessing:** Often used as a preprocessing step before applying other machine learning algorithms.\n",
    "\n",
    "### 5. Example Scenario\n",
    "\n",
    "* **Scenario:** A dataset of customer demographics (age, income, education, spending habits, etc.) with many features. We want to reduce the dimensionality to visualize the customer segments or to feed into a clustering algorithm more efficiently.\n",
    "* **Application:** PCA could reduce these 10+ features into 2 or 3 principal components, allowing us to plot customers on a 2D/3D scatter plot and observe natural groupings. These components might represent underlying customer \"types\" (e.g., \"high-value spenders,\" \"budget-conscious young adults\").\n",
    "\n",
    "### 6. Conceptual Code Snippets (Python)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "4nJg_AVLxNVq"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Sample data (conceptual)\n",
    "# data = pd.DataFrame({\n",
    "#     'Feature1': np.random.rand(100) * 10,\n",
    "#     'Feature2': np.random.rand(100) * 5,\n",
    "#     'Feature3': np.random.rand(100) * 8,\n",
    "#     'Feature4': np.random.rand(100) * 12\n",
    "# })\n",
    "\n",
    "# 1. Standardize the data\n",
    "# scaler = StandardScaler()\n",
    "# scaled_data = scaler.fit_transform(data)\n",
    "\n",
    "# 2. Initialize PCA\n",
    "# pca = PCA(n_components=2) # Reduce to 2 components for visualization\n",
    "\n",
    "# 3. Fit PCA and transform the data\n",
    "# principal_components = pca.fit_transform(scaled_data)\n",
    "# principal_df = pd.DataFrame(data=principal_components, columns=['PC1', 'PC2'])\n",
    "# print(\"Transformed Data (First 5 rows):\\n\", principal_df.head())\n",
    "\n",
    "# 4. Explained Variance Ratio (important for deciding n_components)\n",
    "# print(\"\\nExplained Variance Ratio for each PC:\\n\", pca.explained_variance_ratio_)\n",
    "# print(f\"Total variance explained by 2 components: {pca.explained_variance_ratio_.sum():.2f}\")\n",
    "\n",
    "# 5. Conceptual Scree Plot (to decide number of components)\n",
    "# pca_full = PCA()\n",
    "# pca_full.fit(scaled_data)\n",
    "# plt.plot(np.cumsum(pca_full.explained_variance_ratio_))\n",
    "# plt.xlabel('Number of Components')\n",
    "# plt.ylabel('Cumulative Explained Variance')\n",
    "# plt.title('Scree Plot')\n",
    "# plt.grid(True)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v_3DtWLMxNVq"
   },
   "source": [
    "### 7. Discussion Points\n",
    "\n",
    "* What is the difference between principal components and original features?\n",
    "* How do you decide on the optimal number of principal components to keep?\n",
    "* What are the limitations of PCA? (e.g., linearity assumption, interpretability of components).\n",
    "* When would you *not* use PCA? (e.g., when interpretability of original features is paramount, or for sparse data where other methods might be better).\n",
    "\n",
    "---\n",
    "\n",
    "## Conceptual: Clustering\n",
    "\n",
    "### Title: Clustering: Finding Natural Groupings in Unlabeled Data\n",
    "\n",
    "### 1. Introduction: The Art of Unsupervised Grouping\n",
    "\n",
    "* **Concept:** Clustering is an unsupervised machine learning task that involves grouping a set of data points such that points in the same group (cluster) are more similar to each other than to those in other groups.\n",
    "* **Unsupervised Learning:** Unlike classification (supervised), there are no predefined labels or target variables. The algorithm discovers the structure directly from the data.\n",
    "* **Applications:** Customer segmentation, anomaly detection, document analysis, image segmentation, biological data analysis.\n",
    "\n",
    "### 2. Common Clustering Algorithms\n",
    "\n",
    "* **A. K-Means Clustering:**\n",
    "    * **Concept:** An iterative algorithm that aims to partition 'n' observations into 'k' clusters, where each observation belongs to the cluster with the nearest mean (centroid).\n",
    "    * **How it Works:**\n",
    "        1.  Initialize K cluster centroids randomly.\n",
    "        2.  Assign each data point to the nearest centroid.\n",
    "        3.  Recalculate new centroids as the mean of all points assigned to that cluster.\n",
    "        4.  Repeat steps 2 and 3 until centroids no longer change significantly or a maximum number of iterations is reached.\n",
    "    * **Strengths:** Simple, relatively fast, scalable for large datasets.\n",
    "    * **Weaknesses:** Requires specifying 'K' beforehand, sensitive to initial centroid placement, assumes spherical clusters of similar size, sensitive to outliers.\n",
    "    * **Relates to:** Distance Metrics (Euclidean is standard), Elbow Method (for choosing K).\n",
    "\n",
    "* **B. Hierarchical Clustering:**\n",
    "    * **Concept:** Builds a hierarchy of clusters.\n",
    "    * **Types:**\n",
    "        * **Agglomerative (Bottom-Up):** Starts with each data point as a single cluster, then successively merges the closest pairs of clusters until all points are in one cluster or a stopping criterion is met.\n",
    "        * **Divisive (Top-Down):** Starts with all points in one cluster and recursively splits them.\n",
    "    * **Output:** A dendrogram (tree-like diagram) that shows the sequence of merges or splits.\n",
    "    * **Strengths:** Doesn't require pre-specifying 'K', produces a hierarchy that can reveal relationships at different levels of granularity.\n",
    "    * **Weaknesses:** Computationally more expensive for large datasets, difficult to handle noisy data and outliers.\n",
    "    * **Relates to:** Distance Metrics, Linkage Criteria (how distance between clusters is defined - single, complete, average, ward).\n",
    "\n",
    "* **C. DBSCAN (Density-Based Spatial Clustering of Applications with Noise):**\n",
    "    * **Concept:** Groups together points that are closely packed together (points with many nearest neighbors), marking as outliers points that lie alone in low-density regions.\n",
    "    * **Strengths:** Can find arbitrarily shaped clusters, robust to outliers, doesn't require specifying 'K' beforehand.\n",
    "    * **Weaknesses:** Difficult to find suitable parameters ($\\epsilon$ and `min_samples`), struggles with varying densities.\n",
    "    * **Key Parameters:** $\\epsilon$ (epsilon): Maximum distance between two samples for one to be considered as in the neighborhood of the other. `min_samples`: The number of samples (or total weight) in a neighborhood for a point to be considered as a core point.\n",
    "\n",
    "### 3. Evaluation of Clustering (Conceptual)\n",
    "\n",
    "* **Intrinsic Measures (Internal):** Evaluate cluster quality based only on the data and the clustering result (e.g., Silhouette Score, Davies-Bouldin Index).\n",
    "* **Extrinsic Measures (External):** Compare clustering results to a known ground truth (if available, which is rare in unsupervised learning) (e.g., Adjusted Rand Index, Mutual Information).\n",
    "\n",
    "### 4. How it Relates\n",
    "\n",
    "* **Distance Metrics:** Fundamental for determining closeness of points.\n",
    "* **PCA:** Often used as a preprocessing step to reduce dimensionality before clustering.\n",
    "* **Gaussian Mixture Models:** Can be seen as a probabilistic alternative to K-Means.\n",
    "\n",
    "### 5. Example Scenario\n",
    "\n",
    "* **Scenario:** Analyzing customer transaction data (e.g., purchase frequency, average order value, product categories). We want to segment customers into distinct groups for targeted marketing.\n",
    "* **Application:** K-Means could identify 3-5 distinct customer segments (e.g., \"high-spenders,\" \"bargain-hunters,\" \"infrequent shoppers\"). Hierarchical clustering could reveal nested segments (e.g., \"loyal customers\" further split into \"loyal luxury buyers\" and \"loyal everyday buyers\").\n",
    "\n",
    "### 6. Conceptual Code Snippets (Python)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "0Jcs32s8xNVr"
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import silhouette_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.cluster.hierarchy as sch # For dendrogram\n",
    "\n",
    "# Sample data (conceptual)\n",
    "# data = pd.DataFrame({\n",
    "#     'FeatureA': np.random.rand(100) * 10,\n",
    "#     'FeatureB': np.random.rand(100) * 8\n",
    "# })\n",
    "\n",
    "# 1. Feature Scaling\n",
    "# scaler = StandardScaler()\n",
    "# scaled_data = scaler.fit_transform(data)\n",
    "\n",
    "# 2. K-Means Clustering\n",
    "# kmeans = KMeans(n_clusters=3, random_state=42, n_init='auto') # n_init='auto' to silence warning in newer versions\n",
    "# kmeans_labels = kmeans.fit_predict(scaled_data)\n",
    "# print(\"K-Means Cluster Labels (First 10):\\n\", kmeans_labels[:10])\n",
    "# print(\"K-Means Silhouette Score:\", silhouette_score(scaled_data, kmeans_labels))\n",
    "\n",
    "# Conceptual Elbow Method for K-Means (to find optimal K)\n",
    "# wcss = [] # Within-cluster sum of squares\n",
    "# for i in range(1, 11):\n",
    "#     kmeans = KMeans(n_clusters=i, random_state=42, n_init='auto')\n",
    "#     kmeans.fit(scaled_data)\n",
    "#     wcss.append(kmeans.inertia_)\n",
    "# plt.plot(range(1, 11), wcss)\n",
    "# plt.title('Elbow Method for K-Means')\n",
    "# plt.xlabel('Number of Clusters (K)')\n",
    "# plt.ylabel('WCSS')\n",
    "# plt.show()\n",
    "\n",
    "# 3. Hierarchical Clustering (Agglomerative)\n",
    "# agg_cluster = AgglomerativeClustering(n_clusters=3)\n",
    "# agg_labels = agg_cluster.fit_predict(scaled_data)\n",
    "# print(\"\\nAgglomerative Clustering Labels (First 10):\\n\", agg_labels[:10])\n",
    "# print(\"Agglomerative Silhouette Score:\", silhouette_score(scaled_data, agg_labels))\n",
    "\n",
    "# Conceptual Dendrogram Plot (for Hierarchical)\n",
    "# plt.figure(figsize=(10, 7))\n",
    "# dend = sch.dendrogram(sch.linkage(scaled_data, method='ward'))\n",
    "# plt.title('Dendrogram for Hierarchical Clustering')\n",
    "# plt.xlabel('Data Points')\n",
    "# plt.ylabel('Euclidean Distances')\n",
    "# plt.show()\n",
    "\n",
    "# 4. DBSCAN Clustering\n",
    "# dbscan = DBSCAN(eps=0.5, min_samples=5) # eps and min_samples need tuning\n",
    "# dbscan_labels = dbscan.fit_predict(scaled_data)\n",
    "# print(\"\\nDBSCAN Cluster Labels (First 10):\\n\", dbscan_labels[:10])\n",
    "# # DBSCAN can have -1 for noise points, so silhouette_score needs careful handling if noise exists\n",
    "# # if len(set(dbscan_labels)) > 1 and -1 not in dbscan_labels:\n",
    "# #     print(\"DBSCAN Silhouette Score:\", silhouette_score(scaled_data, dbscan_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ta28cvj2xNVr"
   },
   "source": [
    "### 7. Discussion Points\n",
    "\n",
    "* When would you choose K-Means over Hierarchical Clustering or DBSCAN?\n",
    "* How do you determine the optimal number of clusters for K-Means?\n",
    "* What are the advantages of DBSCAN for handling noise and arbitrary cluster shapes?\n",
    "* Discuss the challenges of evaluating clustering results.\n",
    "\n",
    "---\n",
    "\n",
    "## Conceptual : Gaussian Mixture Modeling (GMM)\n",
    "\n",
    "### Title: Gaussian Mixture Models: Probabilistic Clustering and Beyond\n",
    "\n",
    "### 1. Introduction: Soft Clustering with Probabilities\n",
    "\n",
    "* **Concept:** Gaussian Mixture Models (GMMs) are a probabilistic model that assumes data points are generated from a mixture of several Gaussian distributions with unknown parameters (mean, covariance, and mixing proportions for each component). Unlike K-Means, GMMs provide *soft assignments* (probabilities) of a data point belonging to each cluster, rather than hard assignments.\n",
    "* **Why use it?**\n",
    "    * **Probabilistic Assignments:** Provides uncertainty in cluster membership.\n",
    "    * **Arbitrary Shapes:** Can model clusters with different sizes, shapes (spherical, elliptical), and orientations, unlike K-Means' spherical assumption.\n",
    "    * **Density Estimation:** Can be used to estimate the underlying probability density function of the data.\n",
    "\n",
    "### 2. How GMM Works (Conceptual Steps)\n",
    "\n",
    "* GMMs use the **Expectation-Maximization (EM) algorithm** to find the optimal parameters for each Gaussian component.\n",
    "    1.  **Initialization (E-step - Estimation of Responsibilities):**\n",
    "        * Randomly initialize the mean, covariance, and mixing proportion for each of the 'K' Gaussian components.\n",
    "        * Calculate the probability (responsibility) that each data point belongs to each component, given the current parameters.\n",
    "    2.  **Maximization (M-step - Maximization of Parameters):**\n",
    "        * Update the parameters (mean, covariance, mixing proportion) of each Gaussian component to maximize the likelihood of the data, based on the responsibilities calculated in the E-step.\n",
    "    3.  **Iteration:** Repeat E-step and M-step until the parameters converge (i.e., the likelihood of the data no longer improves significantly).\n",
    "\n",
    "### 3. Key Concepts\n",
    "\n",
    "* **Components:** The individual Gaussian distributions that make up the mixture model.\n",
    "* **Mixing Proportions (Weights):** The probability of a data point belonging to a particular component (prior probability).\n",
    "* **Mean:** The center of each Gaussian component.\n",
    "* **Covariance Matrix:** Describes the shape, size, and orientation of each Gaussian component. Can be 'spherical', 'tied', 'diag', or 'full'.\n",
    "* **Log-Likelihood:** A measure of how well the model fits the data. EM algorithm aims to maximize this.\n",
    "\n",
    "### 4. How it Relates\n",
    "\n",
    "* **Clustering:** A more flexible and probabilistic alternative to K-Means.\n",
    "* **Probability & Statistics:** Deeply rooted in probability theory and statistical modeling.\n",
    "* **Density Estimation:** Can be used for anomaly detection (low probability points are outliers).\n",
    "\n",
    "### 5. Example Scenario\n",
    "\n",
    "* **Scenario:** Analyzing image pixel data. We want to segment different objects or regions within an image based on their color properties.\n",
    "* **Application:** A GMM could model the distribution of pixel colors, allowing us to identify distinct color \"clusters\" (e.g., sky, grass, water, skin tones) even if they have overlapping distributions. Instead of a hard assignment, each pixel gets a probability of belonging to each color component.\n",
    "\n",
    "### 6. Conceptual Code Snippets (Python)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "SGsbJa0FxNVr"
   },
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Ellipse\n",
    "\n",
    "# Function to draw ellipses for GMM components\n",
    "# def draw_ellipse(position, covariance, ax=None, **kwargs):\n",
    "#     ax = ax or plt.gca()\n",
    "#     if covariance.shape == (2, 2):\n",
    "#         U, s, Vt = np.linalg.svd(covariance)\n",
    "#         angle = np.degrees(np.arctan2(U[1, 0], U[0, 0]))\n",
    "#         width, height = 2 * np.sqrt(s)\n",
    "#     else:\n",
    "#         angle = 0\n",
    "#         width, height = 2 * np.sqrt(covariance)\n",
    "#     for nstd in range(1, 4):\n",
    "#         ax.add_patch(Ellipse(position, nstd * width, nstd * height,\n",
    "#                              angle=angle, **kwargs))\n",
    "\n",
    "# Sample data (conceptual)\n",
    "# np.random.seed(0)\n",
    "# X = np.concatenate([np.random.randn(100, 2) * 2 + [5, 5],\n",
    "#                     np.random.randn(150, 2) * 1.5 + [-5, 0],\n",
    "#                     np.random.randn(70, 2) * 0.5 + [0, -7]])\n",
    "\n",
    "# 1. Feature Scaling (optional, but good practice if scales vary greatly)\n",
    "# scaler = StandardScaler()\n",
    "# scaled_X = scaler.fit_transform(X)\n",
    "\n",
    "# 2. Initialize and Fit GMM\n",
    "# n_components = 3 # Number of Gaussian components\n",
    "# gmm = GaussianMixture(n_components=n_components, random_state=42, covariance_type='full') # 'full', 'tied', 'diag', 'spherical'\n",
    "# gmm.fit(scaled_X)\n",
    "\n",
    "# 3. Get Cluster Assignments (soft assignments / probabilities)\n",
    "# cluster_probabilities = gmm.predict_proba(scaled_X)\n",
    "# print(\"Probabilities of belonging to each cluster (First 5 points):\\n\", cluster_probabilities[:5])\n",
    "\n",
    "# 4. Get Hard Assignments (most probable cluster)\n",
    "# cluster_labels = gmm.predict(scaled_X)\n",
    "# print(\"\\nHard Cluster Labels (First 10):\\n\", cluster_labels[:10])\n",
    "\n",
    "# 5. Visualize GMM (Conceptual for 2D data)\n",
    "# plt.figure(figsize=(8, 6))\n",
    "# plt.scatter(scaled_X[:, 0], scaled_X[:, 1], c=cluster_labels, s=40, cmap='viridis', zorder=2)\n",
    "# for i in range(n_components):\n",
    "#     draw_ellipse(gmm.means_[i], gmm.covariances_[i], alpha=0.5, color='black')\n",
    "# plt.title('GMM Clustering')\n",
    "# plt.xlabel('Scaled Feature 1')\n",
    "# plt.ylabel('Scaled Feature 2')\n",
    "# plt.show()\n",
    "\n",
    "# 6. Evaluate using AIC/BIC for optimal components (Conceptual)\n",
    "# n_components_range = range(1, 7)\n",
    "# aic = []\n",
    "# bic = []\n",
    "# for n in n_components_range:\n",
    "#     gmm = GaussianMixture(n_components=n, random_state=42, n_init='auto')\n",
    "#     gmm.fit(scaled_X)\n",
    "#     aic.append(gmm.aic(scaled_X))\n",
    "#     bic.append(gmm.bic(scaled_X))\n",
    "# plt.plot(n_components_range, aic, label='AIC')\n",
    "# plt.plot(n_components_range, bic, label='BIC')\n",
    "# plt.xlabel('Number of Components')\n",
    "# plt.ylabel('Information Criterion')\n",
    "# plt.title('AIC and BIC for GMM')\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E8kQ2RvTxNVr"
   },
   "source": [
    "### 7. Discussion Points\n",
    "\n",
    "* What are the main advantages of GMMs over K-Means clustering?\n",
    "* Explain the Expectation-Maximization (EM) algorithm in simple terms.\n",
    "* How do you choose the optimal number of components for a GMM? (Introduce AIC/BIC).\n",
    "* Discuss the role of `covariance_type` in GMM and its implications.\n",
    "* When might GMM not be a good choice? (e.g., extremely high dimensionality, non-Gaussian clusters).\n",
    "\n",
    "---\n",
    "\n",
    "## Conceptual: Market Segmentation using Clustering Models\n",
    "\n",
    "### Title: Market Segmentation: Unlocking Business Insights with Clustering\n",
    "\n",
    "### 1. Introduction: Dividing to Conquer\n",
    "\n",
    "* **Concept:** Market segmentation is the process of dividing a broad consumer or business market into sub-groups of consumers (segments) based on some type of shared characteristics.\n",
    "* **Goal:** To understand customer behavior, tailor marketing strategies, develop targeted products/services, and optimize resource allocation.\n",
    "* **Role of Clustering:** Machine learning clustering algorithms are powerful tools for *unsupervised* market segmentation, discovering natural groupings in customer data without prior knowledge of segments.\n",
    "\n",
    "### 2. Why Use Clustering for Market Segmentation?\n",
    "\n",
    "* **Data-Driven:** Segments are identified objectively from data, rather than relying solely on intuition or predefined categories.\n",
    "* **Reveals Hidden Patterns:** Can uncover unexpected customer groups and behaviors.\n",
    "* **Scalability:** Can process large datasets to segment millions of customers.\n",
    "* **Actionable Insights:** Provides a basis for creating personalized customer experiences, pricing strategies, and product development.\n",
    "\n",
    "### 3. Data for Market Segmentation (Typical Features)\n",
    "\n",
    "* **Demographic:** Age, gender, income, education, occupation, marital status.\n",
    "* **Geographic:** Location, climate, population density.\n",
    "* **Psychographic:** Lifestyle, personality, values, interests, attitudes.\n",
    "* **Behavioral:** Purchase history (frequency, recency, monetary value - RFM), website activity, product usage, loyalty, brand interactions.\n",
    "* **Attitudinal:** Preferences, satisfaction, awareness (often collected via surveys).\n",
    "\n",
    "### 4. Process of Market Segmentation with Clustering\n",
    "\n",
    "1.  **Define Business Objective:** What problem are we trying to solve? (e.g., improve campaign ROI, identify high-value customers, personalize product recommendations).\n",
    "2.  **Data Collection & Preparation:**\n",
    "    * Gather relevant customer data.\n",
    "    * Handle missing values, outliers.\n",
    "    * **Feature Engineering:** Create meaningful features (e.g., RFM scores, average spending, time spent on site).\n",
    "    * **Feature Scaling:** Crucial for distance-based clustering algorithms (K-Means, Hierarchical).\n",
    "3.  **Choose Clustering Algorithm:**\n",
    "    * **K-Means:** Simple, efficient, good for clearly separated, spherical clusters. Requires 'K'.\n",
    "    * **Hierarchical Clustering:** Useful for visualizing relationships and exploring different levels of granularity.\n",
    "    * **GMM:** For probabilistic assignments, non-spherical clusters, and density estimation.\n",
    "    * **DBSCAN:** Good for arbitrary shapes and outlier detection, but parameter tuning can be tricky.\n",
    "4.  **Determine Optimal Number of Clusters (if applicable):**\n",
    "    * **K-Means:** Elbow Method, Silhouette Score.\n",
    "    * **GMM:** AIC/BIC.\n",
    "    * Domain expertise is also vital here.\n",
    "5.  **Run Clustering Algorithm:** Apply the chosen algorithm to the prepared data.\n",
    "6.  **Profile and Interpret Clusters:**\n",
    "    * Analyze the characteristics (mean values, distributions) of features within each cluster.\n",
    "    * Give each cluster a meaningful name/persona (e.g., \"The Savvy Savers,\" \"The Luxury Enthusiasts,\" \"The New Explorers\").\n",
    "7.  **Validate and Act on Segments:**\n",
    "    * Are the segments distinct and actionable?\n",
    "    * Develop targeted marketing campaigns, product strategies, or customer service approaches for each segment.\n",
    "    * Monitor the performance of these strategies.\n",
    "\n",
    "### 5. Example Scenario\n",
    "\n",
    "* **Scenario:** An online retail store wants to understand its customer base better to create more effective marketing campaigns.\n",
    "* **Data Features:** Number of orders, average order value, last purchase date (for recency), product categories purchased, Browse time, discount usage.\n",
    "* **Clustering Application:**\n",
    "    * Apply K-Means or GMM to segment customers.\n",
    "    * **Segment 1: \"High-Value Loyalists\"** (High RFM, frequent purchases, high AOV, respond well to loyalty programs).\n",
    "    * **Segment 2: \"Bargain Hunters\"** (High discount usage, low AOV, infrequent but large purchases when sales occur).\n",
    "    * **Segment 3: \"New Explorers\"** (Recent sign-ups, low initial purchase, Browse many categories).\n",
    "    * **Actionable Insights:**\n",
    "        * **High-Value Loyalists:** Offer exclusive previews, personalized recommendations, premium support.\n",
    "        * **Bargain Hunters:** Send targeted promotions during sales, flash deals.\n",
    "        * **New Explorers:** Onboarding campaigns, product discovery guides, small welcome discounts.\n",
    "\n",
    "### 6. Conceptual Code Snippets (Python)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PfVCAj_RxNVs"
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Sample synthetic customer data (conceptual)\n",
    "# For a real scenario, this would be loaded from a database or CSV\n",
    "# np.random.seed(42)\n",
    "# data = pd.DataFrame({\n",
    "#     'Age': np.random.randint(18, 70, 200),\n",
    "#     'Annual_Income': np.random.randint(20000, 150000, 200),\n",
    "#     'Spending_Score': np.random.randint(1, 100, 200),\n",
    "#     'Purchase_Frequency': np.random.randint(1, 10, 200),\n",
    "#     'Avg_Order_Value': np.random.randint(50, 500, 200)\n",
    "# })\n",
    "\n",
    "# Introduce some hidden structure to make clusters more apparent conceptually\n",
    "# data.loc[data['Age'] < 30, 'Spending_Score'] = np.random.randint(60, 100, (data['Age'] < 30).sum())\n",
    "# data.loc[data['Age'] >= 50, 'Spending_Score'] = np.random.randint(10, 50, (data['Age'] >= 50).sum())\n",
    "# data.loc[data['Annual_Income'] > 100000, 'Avg_Order_Value'] = np.random.randint(300, 700, (data['Annual_Income'] > 100000).sum())\n",
    "\n",
    "# 1. Feature Scaling\n",
    "# features_for_clustering = ['Age', 'Annual_Income', 'Spending_Score', 'Purchase_Frequency', 'Avg_Order_Value']\n",
    "# X = data[features_for_clustering]\n",
    "# scaler = StandardScaler()\n",
    "# X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# 2. Determine Optimal K (using Elbow Method - conceptual)\n",
    "# wcss = []\n",
    "# for i in range(1, 11):\n",
    "#     kmeans = KMeans(n_clusters=i, random_state=42, n_init='auto')\n",
    "#     kmeans.fit(X_scaled)\n",
    "#     wcss.append(kmeans.inertia_)\n",
    "# plt.plot(range(1, 11), wcss)\n",
    "# plt.title('Elbow Method for Customer Segmentation')\n",
    "# plt.xlabel('Number of Clusters (K)')\n",
    "# plt.ylabel('WCSS')\n",
    "# plt.show()\n",
    "# Based on the elbow, let's assume K=3 or K=4\n",
    "\n",
    "# 3. Apply K-Means Clustering\n",
    "# optimal_k = 3\n",
    "# kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init='auto')\n",
    "# data['Cluster'] = kmeans.fit_predict(X_scaled)\n",
    "# print(\"Customer Data with Cluster Labels (First 5 rows):\\n\", data.head())\n",
    "\n",
    "# 4. Profile Clusters (Conceptual - using descriptive statistics)\n",
    "# cluster_profiles = data.groupby('Cluster')[features_for_clustering].mean()\n",
    "# print(\"\\nCluster Profiles (Mean of Features):\\n\", cluster_profiles)\n",
    "\n",
    "# Optional: Visualize clusters (e.g., using PCA for 2D visualization)\n",
    "# pca = PCA(n_components=2)\n",
    "# pca_components = pca.fit_transform(X_scaled)\n",
    "# data['PC1'] = pca_components[:, 0]\n",
    "# data['PC2'] = pca_components[:, 1]\n",
    "\n",
    "# plt.figure(figsize=(10, 7))\n",
    "# sns.scatterplot(x='PC1', y='PC2', hue='Cluster', data=data, palette='viridis', s=100, alpha=0.8)\n",
    "# plt.title('Customer Segments (PCA Reduced)')\n",
    "# plt.xlabel('Principal Component 1')\n",
    "# plt.ylabel('Principal Component 2')\n",
    "# plt.legend(title='Cluster')\n",
    "# plt.grid(True)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hVBx0XASxNVs"
   },
   "source": [
    "### 7. Discussion Points\n",
    "\n",
    "* What types of data are most valuable for market segmentation?\n",
    "* How do you interpret the resulting clusters and assign meaningful names?\n",
    "* What are the challenges in implementing market segmentation (e.g., data quality, actionability)?\n",
    "* How do you measure the success of market segmentation efforts?\n",
    "* Beyond marketing, what other business applications can benefit from clustering?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mgarRCY9xNVs"
   },
   "source": [
    "<div class=\"md-recitation\">\n",
    "  Sources\n",
    "  <ol>\n",
    "  <li><a href=\"https://github.com/JacobHonore/Codecademy-Date-A-Scientist\">https://github.com/JacobHonore/Codecademy-Date-A-Scientist</a></li>\n",
    "  <li><a href=\"https://github.com/andrelucas97/projetos-facul\">https://github.com/andrelucas97/projetos-facul</a></li>\n",
    "  <li><a href=\"https://github.com/MSSolanki/KNN_Batweb\">https://github.com/MSSolanki/KNN_Batweb</a></li>\n",
    "  <li><a href=\"https://www.fynd.academy/blog/movie-recommendation-system\">https://www.fynd.academy/blog/movie-recommendation-system</a></li>\n",
    "  <li><a href=\"https://github.com/nazishjaveed/Encryptix-Company-AI-Internship-Project\">https://github.com/nazishjaveed/Encryptix-Company-AI-Internship-Project</a></li>\n",
    "  <li><a href=\"https://github.com/Kuldeep24X7/Stem_Lit_BITS\">https://github.com/Kuldeep24X7/Stem_Lit_BITS</a></li>\n",
    "  <li><a href=\"https://github.com/cefet-rj-dal/dal\">https://github.com/cefet-rj-dal/dal</a></li>\n",
    "  <li><a href=\"https://github.com/ATPAustinPeng/house-price-predictor\">https://github.com/ATPAustinPeng/house-price-predictor</a></li>\n",
    "  <li><a href=\"https://github.com/jbjoannic-keio/covizReport\">https://github.com/jbjoannic-keio/covizReport</a></li>\n",
    "  <li><a href=\"https://github.com/adeniyiopeyemi25/Car-price-prediction-with-Regression-Models-A-case-study-of-AUTOTRADER\">https://github.com/adeniyiopeyemi25/Car-price-prediction-with-Regression-Models-A-case-study-of-AUTOTRADER</a></li>\n",
    "  <li><a href=\"https://www.mql5.com/en/articles/14760\">https://www.mql5.com/en/articles/14760</a></li>\n",
    "  <li><a href=\"https://www.vertica.com/python/documentation_last/learn/BisectingKMeans/\">https://www.vertica.com/python/documentation_last/learn/BisectingKMeans/</a></li>\n",
    "  <li><a href=\"https://github.com/glasgowlab/MAGPIE\">https://github.com/glasgowlab/MAGPIE</a></li>\n",
    "  <li><a href=\"https://github.com/BB5030/programfiles\">https://github.com/BB5030/programfiles</a></li>\n",
    "  <li><a href=\"https://github.com/Drlordbasil/project-name--automated-sales-lead-generation-and-analysis--project-description--the-autom1690330749\">https://github.com/Drlordbasil/project-name--automated-sales-lead-generation-and-analysis--project-description--the-autom1690330749</a></li>\n",
    "  <li><a href=\"https://github.com/accolombini/PROSPECACAO\">https://github.com/accolombini/PROSPECACAO</a></li>\n",
    "  <li><a href=\"https://blog.csdn.net/htuhxf/article/details/107775708\">https://blog.csdn.net/htuhxf/article/details/107775708</a></li>\n",
    "  <li><a href=\"https://library.fiveable.me/principles-management/unit-8/3-firms-external-macro-environment-pestel/study-guide/jaBOcmbUKAVG9V9j\">https://library.fiveable.me/principles-management/unit-8/3-firms-external-macro-environment-pestel/study-guide/jaBOcmbUKAVG9V9j</a></li>\n",
    "  <li><a href=\"https://bits-f464.github.io/pages/labs/lab_9/4_DBSCAN.html\">https://bits-f464.github.io/pages/labs/lab_9/4_DBSCAN.html</a></li>\n",
    "  <li><a href=\"https://github.com/nayakatul/Churn-Prediction\">https://github.com/nayakatul/Churn-Prediction</a></li>\n",
    "  </ol>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
