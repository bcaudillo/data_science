{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Preprocessing Lab: Hotel Reviews\n",
    "\n",
    "## Overview\n",
    "\n",
    "You've recently joined TextInsight, a customer feedback analysis company that helps businesses understand customer sentiments across various platforms. As a junior data scientist, your first project involves preprocessing a dataset of hotel reviews that will be used to build a sentiment analysis model.\n",
    "\n",
    "The hotel chain wants to identify common issues mentioned in negative reviews and highlight positive experiences that could be promoted in their marketing. Before any analysis can be performed, the raw text data needs to be cleaned and transformed into a structured format suitable for NLP algorithms.\n",
    "\n",
    "This lab will guide you through implementing a text preprocessing pipeline following these key steps:\n",
    "\n",
    "- Text Acquisition and Initial Cleaning\n",
    "- Tokenization and Structural Decomposition\n",
    "- Normalization and Standardization\n",
    "- Noise Filtering and Feature Selection\n",
    "- Feature Extraction and Representation\n",
    "- Validation and Iterative Refinement\n",
    "\n",
    "You'll apply these steps to transform messy, unstructured hotel reviews into clean, analyzable data that can reveal meaningful patterns and insights.\n",
    "\n",
    "## Step 0: Imports and Data\n",
    "\n",
    "First, run the provided code cell to import the necessary libraries and download required NLTK resources:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell without changes\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "\n",
    "# Import specific NLTK modules\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.probability import FreqDist\n",
    "from nltk import pos_tag\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from nltk.util import ngrams\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "# Load the hotel reviews dataset\n",
    "hotel_reviews = pd.read_csv('hotel_reviews.csv')\n",
    "hotel_reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell without changes\n",
    "# Calculate basic text statistics\n",
    "hotel_reviews['review_length'] = hotel_reviews['review_text'].apply(len)\n",
    "hotel_reviews['word_count'] = hotel_reviews['review_text'].apply(lambda x: len(str(x).split()))\n",
    "hotel_reviews['sentence_count'] = hotel_reviews['review_text'].apply(lambda x: len(sent_tokenize(str(x))))\n",
    "\n",
    "# Create visualizations to understand the data\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.histplot(hotel_reviews['word_count'], bins=20)\n",
    "plt.title('Distribution of Review Lengths')\n",
    "plt.xlabel('Word Count')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.boxplot(x='rating', y='word_count', data=hotel_reviews)\n",
    "plt.title('Review Length by Rating')\n",
    "plt.xlabel('Rating (1-5)')\n",
    "plt.ylabel('Word Count')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Text Cleaning and Tokenization\n",
    "Implement functions to clean the text and break it down into analyzable units:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the reviews into sentences and word tokens\n",
    "hotel_reviews['sentences'] = None\n",
    "hotel_reviews['tokens'] = None\n",
    "\n",
    "# Assign the count of unique word tokens across all reviews\n",
    "all_tokens = []\n",
    "for tokens in hotel_reviews['tokens']:\n",
    "    None\n",
    "unique_token_count_pre = len(None)\n",
    "\n",
    "# Create function to clean initial word tokens\n",
    "def clean_text(text_tokens):\n",
    "    \"\"\"\n",
    "    Clean raw text by:\n",
    "    1. Converting to lowercase\n",
    "    2. Removing HTML tags if present\n",
    "    3. Removing special characters\n",
    "    4. Handling common abbreviations\n",
    "    \n",
    "    Parameters:\n",
    "    text_tokens: list of word tokens\n",
    "    \n",
    "    Returns:\n",
    "    list: cleaned list of tokens\n",
    "    \"\"\"\n",
    "    # Convert to lowercase\n",
    "    tokens = None\n",
    "    \n",
    "    # Remove punctuation but keep numbers\n",
    "    cleaned = None\n",
    "    \n",
    "    return cleaned\n",
    "\n",
    "# Apply cleaning to all reviews\n",
    "hotel_reviews['clean_tokens'] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell without changes\n",
    "# Check results on a sample review\n",
    "sample_idx = 10  # Review at index 10\n",
    "print(\"Original review:\")\n",
    "print(hotel_reviews['review_text'][sample_idx])\n",
    "print(\"\\nSentences:\")\n",
    "print(hotel_reviews['sentences'][sample_idx])\n",
    "print(\"\\nTokens (first 20):\")\n",
    "print(hotel_reviews['clean_tokens'][sample_idx][:20])\n",
    "print(\"\\nUnique Number of Tokens:\")\n",
    "print(unique_token_count_pre)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Stopword Removal and Normalization\n",
    "Filter out common words that add little analytical value and normalize (lemmatize) the remaining tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get English stopwords as a set\n",
    "stop_words = None\n",
    "\n",
    "# Function to remove stopwords\n",
    "def remove_stopwords(tokens):\n",
    "    \"\"\"\n",
    "    Remove stopwords from a list of tokens\n",
    "    \n",
    "    Parameters:\n",
    "    tokens (list): List of word tokens\n",
    "    \n",
    "    Returns:\n",
    "    list: Tokens with stopwords removed\n",
    "    \"\"\"\n",
    "    return None\n",
    "\n",
    "# Apply stopword removal\n",
    "hotel_reviews['filtered_tokens'] = None\n",
    "\n",
    "# Initialize lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Helper function for lemmatization with POS tagging\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    \"\"\"\n",
    "    Convert NLTK POS tags to WordNet POS tags\n",
    "    \"\"\"\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "# Lemmatization function\n",
    "def lemmatize_tokens(tokens):\n",
    "    \"\"\"\n",
    "    Lemmatize tokens with appropriate POS tags\n",
    "    \n",
    "    Parameters:\n",
    "    tokens (list): List of word tokens\n",
    "    \n",
    "    Returns:\n",
    "    list: Lemmatized tokens\n",
    "    \"\"\"\n",
    "    # Tag tokens with parts of speech\n",
    "    tokens_tagged = None\n",
    "    \n",
    "    # Convert to WordNet POS tags\n",
    "    pos_tokens = None\n",
    "    \n",
    "    # Lemmatize with POS tags\n",
    "    lemmatized = None\n",
    "    \n",
    "    return lemmatized\n",
    "\n",
    "# Apply lemmatization\n",
    "hotel_reviews['lemmatized_tokens'] = None\n",
    "\n",
    "# Calculate stopword reduction percentage\n",
    "# Assign the count of unique word tokens across all reviews after cleaning and normalizing\n",
    "all_tokens = []\n",
    "for tokens in hotel_reviews['lemmatized_tokens']:\n",
    "    all_tokens.extend(tokens)\n",
    "unique_token_count_post = len(set(all_tokens))\n",
    "stopword_reduction_percent = ((unique_token_count_pre - unique_token_count_post) / unique_token_count_pre) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell without changes\n",
    "# Check results on the sample review\n",
    "print(\"Tokens after stopword removal (first 15):\")\n",
    "print(hotel_reviews['filtered_tokens'][sample_idx][:15])\n",
    "print(\"\\nTokens after lemmatization (first 15):\")\n",
    "print(hotel_reviews['lemmatized_tokens'][sample_idx][:15])\n",
    "print(f\"\\nRemoving stopwords and lemmatizing the text has reduced the vocaburlary by {stopword_reduction_percent:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Generate N-grams and Analyze Frequent Terms\n",
    "Extract word sequences to capture phrases and identify common topics in the reviews:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate bigrams and trigrams\n",
    "def generate_ngrams(tokens, n):\n",
    "    \"\"\"\n",
    "    Generate n-grams from a list of tokens\n",
    "    \n",
    "    Parameters:\n",
    "    tokens (list): List of tokens\n",
    "    n (int): Size of n-grams to generate\n",
    "    \n",
    "    Returns:\n",
    "    list: List of n-grams as tuples\n",
    "    \"\"\"\n",
    "    return None\n",
    "\n",
    "# Generate bigrams and trigrams - hint - apply lamdba function\n",
    "hotel_reviews['bigrams'] = None\n",
    "hotel_reviews['trigrams'] = None\n",
    "\n",
    "# Combine all lemmatized tokens into a single list for frequency analysis\n",
    "all_tokens = []\n",
    "for tokens in hotel_reviews['lemmatized_tokens']:\n",
    "    all_tokens.extend(tokens)\n",
    "\n",
    "# Calculate all token frequencies\n",
    "token_freq = None\n",
    "\n",
    "# Get all bigrams for frequency analysis\n",
    "all_bigrams = []\n",
    "for bigrams in hotel_reviews['bigrams']:\n",
    "    all_bigrams.extend(bigrams)\n",
    "\n",
    "# Calculate bigram frequencies\n",
    "bigram_freq = None\n",
    "\n",
    "# Visualize top tokens and bigrams\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.subplot(2, 1, 1)\n",
    "token_freq.plot(20, title=\"Top 20 Words in Reviews\")\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "bigram_freq.plot(20, title=\"Top 20 Bigrams in Reviews\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Find top tokens and bigrams for positive and negative reviews\n",
    "positive_reviews = hotel_reviews[hotel_reviews['rating'] >= 4]\n",
    "negative_reviews = hotel_reviews[hotel_reviews['rating'] <= 2]\n",
    "\n",
    "# Collect tokens from positive and negative reviews\n",
    "positive_tokens = []\n",
    "for tokens in positive_reviews['lemmatized_tokens']:\n",
    "    positive_tokens.extend(tokens)\n",
    "\n",
    "negative_tokens = []\n",
    "for tokens in negative_reviews['lemmatized_tokens']:\n",
    "    negative_tokens.extend(tokens)\n",
    "\n",
    "# Calculate frequencies\n",
    "positive_freq = None\n",
    "negative_freq = None\n",
    "\n",
    "# Visualize top tokens and bigrams\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.subplot(2, 1, 1)\n",
    "positive_freq.plot(20, title=\"Top 20 Words in Positive Reviews\")\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.subplot(2, 1, 1)\n",
    "negative_freq.plot(20, title=\"Top 20 Words in Negative Reviews\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Apply Sentiment Analysis to Preprocessed Text\n",
    "Use the preprocessed text to analyze sentiment and compare with the explicit ratings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize sentiment analyzer\n",
    "sia = None\n",
    "\n",
    "# Join lemmatized tokens back into strings for sentiment analysis\n",
    "hotel_reviews['lemmatized_text'] = None\n",
    "\n",
    "# Calculate sentiment scores\n",
    "hotel_reviews['sentiment_scores'] = None\n",
    "hotel_reviews['sentiment_compound'] = None\n",
    "\n",
    "# Classify sentiment based on compound score\n",
    "def classify_sentiment(score):\n",
    "    \"\"\"\n",
    "    Classify sentiment based on compound score\n",
    "    \n",
    "    Parameters:\n",
    "    score (float): Compound sentiment score\n",
    "    \n",
    "    Returns:\n",
    "    str: Sentiment classification (positive, negative, or neutral)\n",
    "    \"\"\"\n",
    "    if score >= 0.05:\n",
    "        return 'positive'\n",
    "    elif score <= -0.05:\n",
    "        return 'negative'\n",
    "    else:\n",
    "        return 'neutral'\n",
    "\n",
    "hotel_reviews['predicted_sentiment'] = hotel_reviews['sentiment_compound'].apply(classify_sentiment)\n",
    "\n",
    "# Create a new column that compares the predicted sentiment with the actual rating\n",
    "def compare_sentiment_with_rating(row):\n",
    "    \"\"\"\n",
    "    Compare predicted sentiment with rating\n",
    "    \n",
    "    Parameters:\n",
    "    row: DataFrame row\n",
    "    \n",
    "    Returns:\n",
    "    str: Match status (match or mismatch)\n",
    "    \"\"\"\n",
    "    # Convert rating to sentiment category\n",
    "    actual_sentiment = 'positive' if row['rating'] >= 4 else ('negative' if row['rating'] <= 2 else 'neutral')\n",
    "    \n",
    "    # Compare with predicted sentiment - find where they match/equal\n",
    "    if None:\n",
    "        return 'match'\n",
    "    else:\n",
    "        return 'mismatch'\n",
    "\n",
    "hotel_reviews['sentiment_match'] = hotel_reviews.apply(compare_sentiment_with_rating, axis=1) \n",
    "\n",
    "# Calculate sentiment accuracy\n",
    "sentiment_accuracy = (hotel_reviews['sentiment_match'] == 'match').mean() * 100\n",
    "\n",
    "# Analyze mismatches\n",
    "mismatches = None\n",
    "mismatches_by_rating = mismatches['rating'].value_counts().sort_index()\n",
    "mismatches_by_rating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Evaluate the Preprocessing Pipeline\n",
    "Analyze how your preprocessing choices affected the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to measure vocabulary size at each step\n",
    "def calculate_vocabulary_sizes(df):\n",
    "    \"\"\"\n",
    "    Calculate vocabulary size at each preprocessing step\n",
    "    \n",
    "    Parameters:\n",
    "    df (DataFrame): DataFrame with tokens at different preprocessing stages\n",
    "    \n",
    "    Returns:\n",
    "    dict: Vocabulary sizes for each step\n",
    "    \"\"\"\n",
    "    # Original tokens\n",
    "    original_vocab = set()\n",
    "    None\n",
    "    \n",
    "    # After stopword removal\n",
    "    filtered_vocab = set()\n",
    "    None\n",
    "    # After lemmatization\n",
    "    lemmatized_vocab = set()\n",
    "    None\n",
    "    \n",
    "    return {\n",
    "        'original': len(original_vocab),\n",
    "        'filtered': len(filtered_vocab),\n",
    "        'lemmatized': len(lemmatized_vocab)\n",
    "    }\n",
    "\n",
    "vocab_sizes = calculate_vocabulary_sizes(hotel_reviews)\n",
    "\n",
    "# Calculate token reduction percentages\n",
    "original_to_filtered = None\n",
    "filtered_to_lemmatized = None\n",
    "total_reduction = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell without changes\n",
    "# Summarize vocabulary size changes\n",
    "print(f\"Original vocabulary size: {vocab_sizes['original']} unique tokens\")\n",
    "print(f\"After stopword removal: {vocab_sizes['filtered']} unique tokens ({original_to_filtered:.1f}% reduction)\")\n",
    "print(f\"After lemmatization: {vocab_sizes['lemmatized']} unique tokens ({filtered_to_lemmatized:.1f}% further reduction)\")\n",
    "print(f\"Total vocabulary reduction: {total_reduction:.1f}%\")\n",
    "\n",
    "# Create a bar chart showing vocabulary size reduction\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(['Original', 'After Stopwords', 'After Lemmatization'], \n",
    "        [vocab_sizes['original'], vocab_sizes['filtered'], vocab_sizes['lemmatized']])\n",
    "plt.title('Vocabulary Size Reduction through Preprocessing')\n",
    "plt.ylabel('Number of Unique Tokens')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Cohort_Env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
