{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58805c09",
   "metadata": {},
   "source": [
    "# Implementing Neural Networks with Backpropagation and Gradient Descent Lab\n",
    "## Overview\n",
    "You are a newly hired data scientist at HealthTech Innovations, a healthcare analytics company that works with hospitals to improve patient care through data-driven solutions. Your team has been tasked with developing a predictive model to estimate patient length of stay (LOS) at hospitals. This information is critical for resource allocation, staff scheduling, and financial planning.\n",
    "\n",
    "Your supervisor has asked you to build a neural network that can predict the length of stay (in days) based on various patient attributes such as age, diagnosis codes, admission type, insurance type, and several health indicators. The goal is to provide hospital administrators with accurate predictions to optimize resource allocation.\n",
    "\n",
    "This lab will guide you through the process of implementing a neural network using TensorFlow and Keras, focusing specifically on understanding the backpropagation and gradient descent algorithms that power neural network training. You'll follow a similar process to what was demonstrated in the lesson, but with a different dataset and problem context.\n",
    "## Process\n",
    "1. Prepare the data\n",
    "2. Design a neural network architecture\n",
    "3. Configure the optimization process\n",
    "4. Train the models\n",
    "5. Visualize the learning process\n",
    "6. Evaluate performance\n",
    "\n",
    "## Tools and Resources\n",
    "To complete this lab, you will need:\n",
    "\n",
    "- Python 3.6+\n",
    "- The following libraries:\n",
    "    - TensorFlow 2.x\n",
    "    - Keras (included with TensorFlow)\n",
    "    - NumPy\n",
    "    - Pandas\n",
    "    - Matplotlib\n",
    "    - Scikit-learn\n",
    "- Provided Starter Notebook\n",
    "- Provided Dataset File: patient_los.csv\n",
    "\n",
    "## Step 0: Libraries and Data Loading\n",
    "Start by importing all the necessary libraries, setting random seeds for reproducibility, and loading in the provided data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf74211",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "import random\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('patient_los.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba32709",
   "metadata": {},
   "source": [
    "## Step 1: Explore the Patient Length-of-Stay Dataset\n",
    "Begin by understanding the structure and size of your data. Use visuals to help understand basic relationships. The dataset contains information about patients, including demographics, admission details, and health indicators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4bd53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display basic information about the dataset\n",
    "None\n",
    "\n",
    "# Display basic statistics\n",
    "None\n",
    "\n",
    "# Check for missing values\n",
    "None\n",
    "\n",
    "# Visualize the distribution of the target variable (length of stay)\n",
    "None\n",
    "\n",
    "# Explore relationships between features and target\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Admission type\n",
    "plt.subplot(2, 2, 1)\n",
    "None\n",
    "\n",
    "# Age\n",
    "plt.subplot(2, 2, 2)\n",
    "None\n",
    "\n",
    "# Department\n",
    "plt.subplot(2, 2, 3)\n",
    "None\n",
    "\n",
    "# Number of Procedures\n",
    "plt.subplot(2, 2, 4)\n",
    "None\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate correlations with the target variable (numeric only)\n",
    "correlations = None\n",
    "print(\"\\nFeature correlations with length of stay:\")\n",
    "print(correlations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42fda083",
   "metadata": {},
   "source": [
    "## Step 2: Prepare the Data for Training\n",
    "Preprocess the data by splitting into training and testing sets and then create a column transformer pipeline that will one hot encode categorical features and standard scale numeric features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe43bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target\n",
    "X = None\n",
    "y = None\n",
    "\n",
    "# Split the data into training and testing sets (test_size=0.2, random_state=42)\n",
    "None\n",
    "\n",
    "# Get list of numerical and categorical columns\n",
    "numerical_cols = None\n",
    "categorical_cols = None\n",
    "\n",
    "# Create preprocessing pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_cols),\n",
    "        ('cat', OneHotEncoder(drop='first', sparse_output=False), categorical_cols)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Fit the preprocessor on training data only\n",
    "X_train_processed = None\n",
    "# Apply the same transformation to test data\n",
    "X_test_processed = None\n",
    "\n",
    "# Get feature names after one-hot encoding (optional, for reference)\n",
    "ohe_feature_names = []\n",
    "if categorical_cols:\n",
    "    ohe = preprocessor.named_transformers_['cat']\n",
    "    ohe_categories = ohe.categories_\n",
    "    ohe_feature_names = []\n",
    "    for i, category in enumerate(ohe_categories):\n",
    "        cat_col = categorical_cols[i]\n",
    "        # Skip first category (due to drop='first')\n",
    "        for cat in category[1:]:\n",
    "            ohe_feature_names.append(f\"{cat_col}_{cat}\")\n",
    "\n",
    "feature_names = numerical_cols + ohe_feature_names\n",
    "\n",
    "print(f\"Training set shape: {X_train_processed.shape}\")\n",
    "print(f\"Testing set shape: {X_test_processed.shape}\")\n",
    "print(f\"Number of features after encoding: {X_train_processed.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6010dd16",
   "metadata": {},
   "source": [
    "## Step 3: Build a Neural Network Model\n",
    "Design a neural network architecture for the regression task of predicting patient length of stay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776bf9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the number of input features\n",
    "input_dim = None\n",
    "\n",
    "# Build a sequential model\n",
    "model = keras.Sequential([\n",
    "    # Input layer with the appropriate input shape\n",
    "    keras.layers.Input(None),\n",
    "    \n",
    "    # First hidden layer with 64 neurons and ReLU activation\n",
    "    None,\n",
    "    \n",
    "    # Second hidden layer with 32 neurons and ReLU activation\n",
    "    None,\n",
    "    \n",
    "    # Output layer with a single neuron (for regression)\n",
    "    None\n",
    "])\n",
    "\n",
    "# Display the model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161ae2bd",
   "metadata": {},
   "source": [
    "## Step 4: Configure the Training Process\n",
    "Configure the model for training by selecting an optimizer, loss function, and evaluation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b013db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and compile three different models with different optimizers\n",
    "loss = 'mean_squared_error'\n",
    "metric = ['mae']\n",
    "\n",
    "# Model 1: Using SGD (Stochastic Gradient Descent)\n",
    "model_sgd = keras.models.clone_model(model)\n",
    "# Compile\n",
    "None\n",
    "\n",
    "# Model 2: Using RMSprop\n",
    "model_rmsprop = keras.models.clone_model(model)\n",
    "# Compile\n",
    "None\n",
    "\n",
    "# Model 3: Using Adam\n",
    "model_adam = keras.models.clone_model(model)\n",
    "# Compile\n",
    "None\n",
    "\n",
    "# Defined number of epochs and batch size\n",
    "epochs = 50\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f53fe3",
   "metadata": {},
   "source": [
    "## Step 5: Train the Models and Analyze Learning\n",
    "Train the three models with different optimizers and analyze their learning patterns. Make sure to use `validation_split=0.2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48808d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model with SGD\n",
    "history_sgd = None\n",
    "\n",
    "# Train the model with RMSprop\n",
    "history_rmsprop = None\n",
    "\n",
    "# Train the model with Adam\n",
    "history_adam = None\n",
    "\n",
    "# Plot training & validation loss for all three models - include label for each\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Loss comparison\n",
    "plt.subplot(2, 1, 1)\n",
    "# Training loss SGD\n",
    "plt.plot(None)\n",
    "# Validation loss SGD\n",
    "plt.plot(None, linestyle='--')\n",
    "# Training loss RMSProp\n",
    "plt.plot(None)\n",
    "# Validation loss RMSProp\n",
    "plt.plot(None, linestyle='--')\n",
    "# Training loss Adam\n",
    "plt.plot(None)\n",
    "# Validation loss Adam\n",
    "plt.plot(None, linestyle='--')\n",
    "plt.title('Model Loss Comparison')\n",
    "plt.ylabel('Loss (MSE)')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# MAE comparison\n",
    "plt.subplot(2, 1, 2)\n",
    "# Training MAE SGD\n",
    "plt.plot(None)\n",
    "# Validation MAE SGD\n",
    "plt.plot(None, linestyle='--')\n",
    "# Training MAE RMSProp\n",
    "plt.plot(None)\n",
    "# Validation MAE RMSProp\n",
    "plt.plot(None, linestyle='--')\n",
    "# Training MAE Adam\n",
    "plt.plot(None)\n",
    "# Validation MAE Adam\n",
    "plt.plot(None, linestyle='--')\n",
    "\n",
    "plt.title('Model MAE Comparison')\n",
    "plt.ylabel('Mean Absolute Error (days)')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print final metrics for each optimizer\n",
    "print(\"Final metrics:\")\n",
    "print(f\"SGD - Training Loss: {history_sgd.history['loss'][-1]:.4f}, Validation Loss: {history_sgd.history['val_loss'][-1]:.4f}\")\n",
    "print(f\"SGD - Training MAE: {history_sgd.history['mae'][-1]:.4f}, Validation MAE: {history_sgd.history['val_mae'][-1]:.4f}\")\n",
    "print(f\"RMSprop - Training Loss: {history_rmsprop.history['loss'][-1]:.4f}, Validation Loss: {history_rmsprop.history['val_loss'][-1]:.4f}\")\n",
    "print(f\"RMSprop - Training MAE: {history_rmsprop.history['mae'][-1]:.4f}, Validation MAE: {history_rmsprop.history['val_mae'][-1]:.4f}\")\n",
    "print(f\"Adam - Training Loss: {history_adam.history['loss'][-1]:.4f}, Validation Loss: {history_adam.history['val_loss'][-1]:.4f}\")\n",
    "print(f\"Adam - Training MAE: {history_adam.history['mae'][-1]:.4f}, Validation MAE: {history_adam.history['val_mae'][-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4629100b",
   "metadata": {},
   "source": [
    "## Step 6: Evaluate Model Performance on Test Data\n",
    "Select the best-performing model based on validation results, and evaluate it on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647db8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the best model based on validation loss\n",
    "val_losses = [\n",
    "    history_sgd.history['val_loss'][-1],\n",
    "    history_rmsprop.history['val_loss'][-1], \n",
    "    history_adam.history['val_loss'][-1]\n",
    "]\n",
    "\n",
    "best_model_idx = np.argmin(val_losses)\n",
    "best_models = [model_sgd, model_rmsprop, model_adam]\n",
    "best_model = best_models[best_model_idx]\n",
    "best_optimizer = ['SGD', 'RMSprop', 'Adam'][best_model_idx]\n",
    "\n",
    "print(f\"The best performing model used the {best_optimizer} optimizer.\")\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "test_loss, test_mae = None\n",
    "print(f\"\\nTest Loss (MSE): {test_loss:.4f}\")\n",
    "print(f\"Test MAE: {test_mae:.4f} days\")\n",
    "\n",
    "# Make predictions with the best model - hint - make sure to flatten\n",
    "predictions = None\n",
    "errors = None\n",
    "\n",
    "# Plot actual vs predicted values\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test, predictions, alpha=0.5)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\n",
    "plt.xlabel('Actual Length of Stay (days)')\n",
    "plt.ylabel('Predicted Length of Stay (days)')\n",
    "plt.title('Actual vs. Predicted Length of Stay')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Plot error distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(errors, bins=50, alpha=0.7)\n",
    "plt.axvline(x=0, color='r', linestyle='--')\n",
    "plt.xlabel('Prediction Error (days)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Prediction Errors')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Calculate additional error metrics\n",
    "mae = None\n",
    "mse = None\n",
    "rmse = None\n",
    "mean_error = np.mean(errors)\n",
    "median_error = np.median(errors)\n",
    "max_error = np.max(np.abs(errors))\n",
    "\n",
    "print(f\"Mean Absolute Error (MAE): {mae:.2f} days\")\n",
    "print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse:.2f} days\")\n",
    "print(f\"Mean Error: {mean_error:.2f} days\")\n",
    "print(f\"Median Error: {median_error:.2f} days\")\n",
    "print(f\"Maximum Error: {max_error:.2f} days\")\n",
    "\n",
    "# Analyze errors by patient characteristics\n",
    "# Join predictions with original test data for analysis\n",
    "test_indices = y_test.index\n",
    "test_data = data.iloc[test_indices].copy()\n",
    "test_data['predicted_los'] = None\n",
    "test_data['error'] = None\n",
    "test_data['abs_error'] = None\n",
    "\n",
    "# Example: Error by age group\n",
    "test_data['age_group'] = pd.cut(test_data['age'], bins=[0, 18, 35, 50, 65, 100], \n",
    "                               labels=['0-18', '19-35', '36-50', '51-65', '65+'])\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(x='age_group', y='abs_error', data=test_data)\n",
    "plt.title('Prediction Error by Age Group')\n",
    "plt.xlabel('Age Group')\n",
    "plt.ylabel('Absolute Error (days)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b959acbc",
   "metadata": {},
   "source": [
    "## Step 7: Final Evaluation and Analysis\n",
    "Evaluate the performance of your best model and generate key insights for stakeholders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd91a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate performance within different error margins\n",
    "within_1day = None\n",
    "within_2days = None\n",
    "within_3days = None\n",
    "\n",
    "print(\"\\nPrediction Accuracy:\")\n",
    "print(f\"Predictions within 1 day of actual: {within_1day:.1f}%\")\n",
    "print(f\"Predictions within 2 days of actual: {within_2days:.1f}%\")\n",
    "print(f\"Predictions within 3 days of actual: {within_3days:.1f}%\")\n",
    "\n",
    "# Analyze which types of cases are most difficult to predict\n",
    "# Group test cases by quartiles of error magnitude\n",
    "test_data['error_quartile'] = pd.qcut(test_data['abs_error'], 4, labels=['Q1', 'Q2', 'Q3', 'Q4'])\n",
    "\n",
    "# Examine characteristics of the highest error cases\n",
    "high_error_cases = test_data[test_data['error_quartile'] == 'Q4']\n",
    "\n",
    "print(\"\\nCharacteristics of Difficult-to-Predict Cases:\")\n",
    "print(f\"Average age: {high_error_cases['age'].mean():.1f} years\")\n",
    "print(f\"Average length of stay: {high_error_cases['length_of_stay'].mean():.1f} days\")\n",
    "print(f\"Average number of procedures: {high_error_cases['num_procedures'].mean():.1f}\")\n",
    "\n",
    "# Generate insights for stakeholders\n",
    "print(\"\\nKey Insights for Hospital Administrators:\")\n",
    "print(\"1. The model can predict patient length of stay with an average error of \" +\n",
    "      f\"{np.mean(np.abs(errors)):.2f} days.\")\n",
    "print(f\"2. {within_2days:.1f}% of predictions are within 2 days of the actual length of stay.\")\n",
    "print(\"3. The model has similar error for all age groups indicating no systemic age bias.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Cohort_Env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
