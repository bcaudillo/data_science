{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1e529de",
   "metadata": {},
   "source": [
    "# Logistic Regression, Decision Trees, and SVMs: \n",
    "\n",
    "## I. Introduction to Baseline Machine Learning Models \n",
    "**Supervised Learning:** Learn from labeled data to predict class labels.\n",
    "\n",
    "**Models:** Logistic Regression, Decision Trees, and SVMs\n",
    "\n",
    "**Why these three?**\n",
    "- Simple and widely used\n",
    "- Diverse mathematical approaches\n",
    "- Great foundation for ML understanding\n",
    "\n",
    "# Supervised Machine Learning\n",
    "# Unsupervised Machine Learning\n",
    "# Reinforcement Learning\n",
    "# The common Word is Learning -- Statistical Learning\n",
    "-- There is a mathematical Function that runs bedhind the scene to find the best fit of the Data Distribution. \n",
    "\n",
    "# Learning is Key because the function learns from the \"Training Dataset\"\n",
    "# Learn the data distribution and try to predict or a continous variable, classify the categorical or to make clustering or forecast\n",
    "# Validation Dataset\n",
    "# Test Dataset\n",
    "What is the difference between Training dataset, Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc2c58c",
   "metadata": {},
   "source": [
    "## II. Model 1: Logistic Regression \n",
    "\n",
    "### A. Concepts\n",
    "- Despite the name, it's a classification algorithm\n",
    "- Predicts probabilities using the sigmoid function\n",
    "- Classifies based on threshold (typically 0.5)\n",
    "\n",
    "### B. Math Behind (Intuition)\n",
    "- Linear model: `z = w1*x1 + w2*x2 + ... + wn*xn + b`\n",
    "- Sigmoid function: `1 / (1 + exp(-z))` gives a probability\n",
    "- Loss function: Cross-entropy minimized with gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd2e6d40",
   "metadata": {},
   "source": [
    "## III. Model 2: Decision Trees \n",
    "\n",
    "### A. Concepts\n",
    "- Flowchart-like rule-based classification\n",
    "- Consists of root, internal, and leaf nodes\n",
    "- Recursive partitioning\n",
    "\n",
    "### B. Math Behind (Intuition)\n",
    "- Splits use Gini impurity or entropy to maximize purity\n",
    "- Greedy algorithm: chooses best split at each step\n",
    "- No optimization or gradient descent needed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4f286b",
   "metadata": {},
   "source": [
    "## IV. Model 3: Support Vector Machines \n",
    "\n",
    "### A. Concepts\n",
    "- Finds the optimal hyperplane separating classes\n",
    "- Maximizes the margin between support vectors\n",
    "- Kernel trick allows non-linear classification\n",
    "\n",
    "### B. Math Behind (Intuition)\n",
    "- Equation: `w·x + b = 0`\n",
    "- Optimization problem: maximize margin subject to constraints\n",
    "- Kernels: Linear, Polynomial, RBF (Gaussian)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f385b8d",
   "metadata": {},
   "source": [
    "## V. Compare and Contrast \n",
    "\n",
    "| Feature | Logistic Regression | Decision Tree | SVM |\n",
    "|--------|---------------------|----------------|-----|\n",
    "| Concept | Probabilistic linear classifier | Rule-based | Max-margin hyperplane |\n",
    "| Decision Boundary | Linear | Piecewise linear | Linear or Non-linear |\n",
    "| Interpretability | Moderate | High | Low |\n",
    "| Data Scaling | Beneficial | Not needed | Needed |\n",
    "| Outlier Handling | Sensitive | Robust | Robust |\n",
    "| Non-linearity | Needs feature engineering | Natural | Kernel trick |\n",
    "| Computational Cost | Fast | Fast (small trees) | High |\n",
    "| Use Case | Binary classification | Rule-based insights | Complex separation |\n",
    "| Strength | Probabilistic & efficient | Interpretable | Effective in high dimension |\n",
    "| Weakness | Assumes linearity | Overfitting risk | Kernel tuning complexity |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb79e973",
   "metadata": {},
   "source": [
    "## VI. Conclusion \n",
    "\n",
    "- **Logistic Regression**: Simple, efficient, interpretable for linear problems\n",
    "- **Decision Trees**: Highly interpretable, visual, good for segmentation\n",
    "- **SVMs**: Robust for high-dimensional and non-linear tasks\n",
    "\n",
    "There’s no one-size-fits-all. Test multiple models and evaluate performance based on data and use-case requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b957773",
   "metadata": {},
   "outputs": [],
   "source": [
    "may@flatironschool.com"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
